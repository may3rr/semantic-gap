{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: D:\\project\\WWW2021-master\\dataset\\RumourEval-19-edit\\train_cleaned.json\n",
      "Processed and saved: D:\\project\\WWW2021-master\\dataset\\RumourEval-19-edit\\val_cleaned.json\n",
      "Processed and saved: D:\\project\\WWW2021-master\\dataset\\RumourEval-19-edit\\test_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_comments(comments):\n",
    "    cleaned_comments = []\n",
    "    for comment in comments:\n",
    "        # 移除 @用户名 和换行符\n",
    "        cleaned_comment = re.sub(r'@\\w+', '', comment)\n",
    "        cleaned_comment = cleaned_comment.replace('\\n', '')\n",
    "        cleaned_comment = cleaned_comment.strip()\n",
    "        if cleaned_comment:\n",
    "            cleaned_comments.append(cleaned_comment)\n",
    "    return cleaned_comments\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    for item in data:\n",
    "        item['comments'] = clean_comments(item['comments'])\n",
    "    new_file_path = file_path.replace('.json', '_cleaned.json')\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Processed and saved: {new_file_path}\")\n",
    "\n",
    "def main(folder_path):\n",
    "    files = ['train.json', 'val.json', 'test.json']\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            process_file(file_path)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r'D:\\project\\WWW2021-master\\dataset\\RumourEval-19-edit'  # 修改为您的文件夹路径\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "-------------------- [2024-03-19 10:25:25] Processing the dataset: RumourEval-19-2 --------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b69ecd1fe1b4709b27ce52d83a50331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train dataset:   0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0177e475bf543f093b234fd54a0935f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test dataset:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549bc8202ece430596fe574fbafba20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val dataset:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# 初始化 BERT 分词器和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(model)\n",
    "def bert_embed(text, max_length=128):\n",
    "    # 对文本进行编码，并限制最大长度\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # 返回BERT模型的最后一层隐藏状态\n",
    "    return outputs.last_hidden_state.squeeze(0)\n",
    "def process_batch(data_batch, batch_index, save_dir, max_comments=5000):\n",
    "    processed_batch_list = []\n",
    "\n",
    "    # 对每个批次的数据进行处理\n",
    "    for data in data_batch:\n",
    "        # 计算内容的嵌入表示\n",
    "        content_embedding = bert_embed(data['content']).to('cuda')\n",
    "        #print(f'Content embedding shape: {content_embedding.shape}')\n",
    "\n",
    "        # 检查评论数量，如果超过5000则仅使用前5000条\n",
    "        comments = data['comments'][:max_comments]\n",
    "\n",
    "        # 计算评论的嵌入表示\n",
    "        comments_embeddings = torch.stack([bert_embed(comment).to('cuda') for comment in comments])\n",
    "        #print(f'Comments embeddings shape: {comments_embeddings.shape}')\n",
    "\n",
    "        # 计算平均池化和最大池化特征\n",
    "        mean_pooling = torch.mean(comments_embeddings, dim=0)\n",
    "        #print(f'Mean pooling shape: {mean_pooling.shape}')\n",
    "        max_pooling = torch.max(comments_embeddings, dim=0).values\n",
    "        #print(f'Max pooling shape: {max_pooling.shape}')\n",
    "\n",
    "        # 计算语义差特征\n",
    "        semantic_gap_mean = content_embedding - mean_pooling\n",
    "        #print(f'Semantic gap mean shape: {semantic_gap_mean.shape}')\n",
    "        semantic_gap_max = content_embedding - max_pooling\n",
    "        #print(f'Semantic gap max shape: {semantic_gap_max.shape}')\n",
    "\n",
    "        # 连接所有特征形成最终特征\n",
    "        final_feature = torch.cat([content_embedding, mean_pooling, max_pooling, semantic_gap_mean, semantic_gap_max])\n",
    "        #print(f'Final feature shape: {final_feature.shape}')\n",
    "        \n",
    "        processed_batch_list.append(final_feature)\n",
    "\n",
    "    # 将处理后的批次数据保存到文件\n",
    "    batch_file_name = f'batch_{batch_index}.npy'\n",
    "    batch_file_path = os.path.join(save_dir, batch_file_name)\n",
    "\n",
    "    # Move the stacked tensor to CPU before converting to NumPy\n",
    "    np.save(batch_file_path, torch.stack(processed_batch_list).cpu().numpy())\n",
    "\n",
    "    # 清理内存\n",
    "    torch.cuda.empty_cache()\n",
    "    del processed_batch_list\n",
    "    return batch_file_name\n",
    "\n",
    "\n",
    "\n",
    "def merge_batches(file_list, output_file_path):\n",
    "    # 合并所有批次文件中的数据\n",
    "    batch_data = [np.load(file) for file in file_list]\n",
    "    merged_data = np.concatenate(batch_data, axis=0)\n",
    "    # 将合并后的数据保存到一个文件\n",
    "    np.save(output_file_path, merged_data)\n",
    "\n",
    "# 设置批处理大小\n",
    "batch_size = 1\n",
    "save_dir = './data'\n",
    "# 创建保存目录\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "datasets_ch = ['RumourEval-19-2']\n",
    "for dataset in datasets_ch:\n",
    "    print(f'\\n\\n{\"-\"*20} [{time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}] Processing the dataset: {dataset} {\"-\"*20}\\n')\n",
    "    # 指定数据集目录\n",
    "    data_dir = os.path.join('../../dataset', dataset)\n",
    "    output_dir = os.path.join(save_dir, dataset)\n",
    "    # 创建输出目录\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    processed_data_dir = os.path.join(output_dir, 'processed')\n",
    "    # 创建处理后数据的保存目录\n",
    "    if not os.path.exists(processed_data_dir):\n",
    "        os.mkdir(processed_data_dir)\n",
    "    # 加载数据集的训练、验证和测试部分\n",
    "    split_datasets = {\n",
    "        t: json.load(open(os.path.join(data_dir, f'{t}.json'), 'r', encoding='utf-8'))\n",
    "        for t in ['train','test','val']\n",
    "    }\n",
    "\n",
    "    for split, data in split_datasets.items():\n",
    "        # 为每个数据分割设置批处理文件目录\n",
    "        batch_dir = os.path.join(processed_data_dir, split)\n",
    "        if not os.path.exists(batch_dir):\n",
    "            os.mkdir(batch_dir)\n",
    "\n",
    "        file_list = []\n",
    "        # 处理数据并保存为批次文件\n",
    "        for batch_index in tqdm(range(0, len(data), batch_size), desc=f\"Processing {split} dataset\"):\n",
    "            data_batch = data[batch_index:batch_index + batch_size]\n",
    "            batch_file_name = process_batch(data_batch, batch_index // batch_size, batch_dir)\n",
    "            file_list.append(os.path.join(batch_dir, batch_file_name))\n",
    "\n",
    "        # 合并所有批次文件为一个单独的文件\n",
    "        final_file_path = os.path.join(processed_data_dir, f'{split}.npy')\n",
    "        merge_batches(file_list, final_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': [\"No.\\n\\nNo matter how hard or focused you wish for something, sometimes it just doesn't happen.\",\n",
       "  'Yes , kind of true, somewhat like destiny,  but I am not talking about a scene of a magic lamp and a genie. Some things are simply impossible but Ive heard people at a good level in meditation can \"make\" things happen and obviously prophets can make a lot of things happen.',\n",
       "  'That\\'s not at all obvious. In fact, I have yet to see proof meditation makes anything happen outside of your mind and body and am pretty sure \"prophets\" are just charismatic leaders.',\n",
       "  \"Haven't you heard about people manipulating matter with their own aura ?\",\n",
       "  \"I have, but that doesn't mean it's true.\",\n",
       "  'Does anyone have any info or experience with this ?',\n",
       "  'Things only happen if you actively do them. ',\n",
       "  \"Two ways this can work. A, you learn to appreciate what you have through introspection. B, you gain the confidence to improve by coming to terms with your issues and facing them.\\n\\nThat's really what meditation is all about. Not space magic. \"],\n",
       " 'content': 'Is it true that in the theta state of meditation, what you yearn/wish for comes true in general?',\n",
       " 'content_emotions_labels': {'anger': 0.0,\n",
       "  'anticipation': 0.0,\n",
       "  'disgust': 0.0,\n",
       "  'fear': 0.0,\n",
       "  'joy': 1.0,\n",
       "  'sadness': 0.0,\n",
       "  'surprise': 0.0,\n",
       "  'trust': 0.0},\n",
       " 'content_emotions_probs': {'anger': 0.07779739797115326,\n",
       "  'anticipation': 0.04301946610212326,\n",
       "  'disgust': 0.055976446717977524,\n",
       "  'fear': 0.012341899797320366,\n",
       "  'joy': 0.29545190930366516,\n",
       "  'sadness': 0.09565771371126175,\n",
       "  'surprise': 0.0010137018980458379,\n",
       "  'trust': 0.1391817033290863},\n",
       " 'id': '6jk9iq',\n",
       " 'label': 'fake',\n",
       " 'comments100_emotions_labels_mean_pooling': {'anger': 0.0,\n",
       "  'anticipation': 0.0,\n",
       "  'disgust': 0.0,\n",
       "  'fear': 0.0,\n",
       "  'joy': 0.75,\n",
       "  'sadness': 0.375,\n",
       "  'surprise': 0.0,\n",
       "  'trust': 0.625},\n",
       " 'comments100_emotions_labels_max_pooling': {'anger': 0.0,\n",
       "  'anticipation': 0.0,\n",
       "  'disgust': 0.0,\n",
       "  'fear': 0.0,\n",
       "  'joy': 1.0,\n",
       "  'sadness': 1.0,\n",
       "  'surprise': 0.0,\n",
       "  'trust': 1.0},\n",
       " 'comments100_emotions_probs_mean_pooling': {'anger': 0.03418830339796841,\n",
       "  'anticipation': 0.051599848782643676,\n",
       "  'disgust': 0.0372273150132969,\n",
       "  'fear': 0.007232318515889347,\n",
       "  'joy': 0.3810464981943369,\n",
       "  'sadness': 0.25387053471058607,\n",
       "  'surprise': 0.0014335436862893403,\n",
       "  'trust': 0.20277809910476208},\n",
       " 'comments100_emotions_probs_max_pooling': {'anger': 0.07444855570793152,\n",
       "  'anticipation': 0.0670710876584053,\n",
       "  'disgust': 0.09663525968790054,\n",
       "  'fear': 0.013828410767018795,\n",
       "  'joy': 0.5792060494422913,\n",
       "  'sadness': 0.4685465395450592,\n",
       "  'surprise': 0.0034140374045819044,\n",
       "  'trust': 0.31136074662208557}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: 读取 train.json 文件\n",
    "file_path = r'D:\\project\\WWW2021-master\\dataset\\RumourEval-19\\test.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 简单查看数据结构\n",
    "data_sample = data[0] if data else \"No data found\"\n",
    "\n",
    "data_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 81/81 [00:10<00:00,  7.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # 引入tqdm\n",
    "\n",
    "# 检查CUDA设备的可用性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased').to(device)  # 将模型移至CUDA设备\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "def get_embedding(text):\n",
    "    # 对文本进行编码并计算嵌入\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = inputs.to(device)  # 将输入数据移至CUDA设备\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(1)  # 取平均作为嵌入\n",
    "    return embeddings.cpu()  # 将嵌入移回CPU\n",
    "\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    # 计算两个嵌入之间的余弦相似度\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    return cos_sim.item()\n",
    "\n",
    "# 对数据进行处理\n",
    "embeddings = []\n",
    "for item in tqdm(data, desc=\"Processing items\"):  # 使用tqdm显示进度条\n",
    "    content_embedding = get_embedding(item['content'])\n",
    "    comment_embeddings = torch.stack([get_embedding(comment) for comment in item['comments']])\n",
    "    avg_comment_embedding = torch.mean(comment_embeddings, 0)\n",
    "    max_comment_embedding, _ = torch.max(comment_embeddings, 0)\n",
    "    avg_similarity = compute_cosine_similarity(content_embedding, avg_comment_embedding)\n",
    "    max_similarity = compute_cosine_similarity(content_embedding, max_comment_embedding)\n",
    "    features = torch.cat((content_embedding.squeeze(), avg_comment_embedding.squeeze(), max_comment_embedding.squeeze(), torch.tensor([avg_similarity, max_similarity])), 0)\n",
    "    embeddings.append(features.numpy())\n",
    "\n",
    "np.save('test_features.npy', np.array(embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m---> 58\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     61\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\WINDOWS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\WINDOWS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mBiGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# 乘以2也是因为双向\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# 取序列的最后一个时间步\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\WINDOWS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\WINDOWS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WINDOWS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1080\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1081\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1082\u001b[0m         hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 定义BiGRU模型\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 乘以2是因为BiGRU的输出是双向的\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)  # 乘以2也是因为双向\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # 取序列的最后一个时间步\n",
    "        return out\n",
    "\n",
    "# 加载数据\n",
    "train_features_path = 'D:/project/WWW2021-master/code/preprocess/train_features.npy'\n",
    "train_labels_path = 'D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/train_(327, 3).npy'\n",
    "test_features_path = 'D:/project/WWW2021-master/code/preprocess/test_features.npy'\n",
    "test_labels_path = 'D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/test_(81, 3).npy'\n",
    "\n",
    "X_train = np.load(train_features_path)\n",
    "y_train = np.argmax(np.load(train_labels_path), axis=1)  # 假设标签是one-hot编码的，我们需要将其转换为标签索引\n",
    "X_test = np.load(test_features_path)\n",
    "y_test = np.argmax(np.load(test_labels_path), axis=1)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # 或根据需要调整\n",
    "num_classes = 3  # 根据实际类别数调整\n",
    "model = BiGRU(input_size, hidden_size, num_classes).to(X_train.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10  # 或根据需要调整\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(classification_report(y_test.numpy(), np.array(all_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 1.0552\n",
      "Epoch [2/2000], Loss: 1.0522\n",
      "Epoch [3/2000], Loss: 0.8714\n",
      "Epoch [4/2000], Loss: 1.0970\n",
      "Epoch [5/2000], Loss: 0.9160\n",
      "Epoch [6/2000], Loss: 0.9013\n",
      "Epoch [7/2000], Loss: 0.7918\n",
      "Epoch [8/2000], Loss: 0.9183\n",
      "Epoch [9/2000], Loss: 0.7551\n",
      "Epoch [10/2000], Loss: 0.6199\n",
      "Epoch [11/2000], Loss: 0.7495\n",
      "Epoch [12/2000], Loss: 0.5572\n",
      "Epoch [13/2000], Loss: 0.5054\n",
      "Epoch [14/2000], Loss: 0.9316\n",
      "Epoch [15/2000], Loss: 0.7213\n",
      "Epoch [16/2000], Loss: 0.4775\n",
      "Epoch [17/2000], Loss: 0.7812\n",
      "Epoch [18/2000], Loss: 0.5992\n",
      "Epoch [19/2000], Loss: 0.4015\n",
      "Epoch [20/2000], Loss: 0.6229\n",
      "Epoch [21/2000], Loss: 0.5669\n",
      "Epoch [22/2000], Loss: 0.4194\n",
      "Epoch [23/2000], Loss: 0.6103\n",
      "Epoch [24/2000], Loss: 0.6210\n",
      "Epoch [25/2000], Loss: 0.6792\n",
      "Epoch [26/2000], Loss: 0.8780\n",
      "Epoch [27/2000], Loss: 0.4889\n",
      "Epoch [28/2000], Loss: 0.3490\n",
      "Epoch [29/2000], Loss: 0.6178\n",
      "Epoch [30/2000], Loss: 0.4503\n",
      "Epoch [31/2000], Loss: 0.3919\n",
      "Epoch [32/2000], Loss: 0.2416\n",
      "Epoch [33/2000], Loss: 0.3922\n",
      "Epoch [34/2000], Loss: 0.2710\n",
      "Epoch [35/2000], Loss: 0.2646\n",
      "Epoch [36/2000], Loss: 0.3631\n",
      "Epoch [37/2000], Loss: 0.6773\n",
      "Epoch [38/2000], Loss: 0.3259\n",
      "Epoch [39/2000], Loss: 0.3172\n",
      "Epoch [40/2000], Loss: 0.3049\n",
      "Epoch [41/2000], Loss: 0.2119\n",
      "Epoch [42/2000], Loss: 0.2446\n",
      "Epoch [43/2000], Loss: 0.2833\n",
      "Epoch [44/2000], Loss: 0.3241\n",
      "Epoch [45/2000], Loss: 0.0750\n",
      "Epoch [46/2000], Loss: 0.3684\n",
      "Epoch [47/2000], Loss: 0.2042\n",
      "Epoch [48/2000], Loss: 0.1425\n",
      "Epoch [49/2000], Loss: 0.3048\n",
      "Epoch [50/2000], Loss: 0.1923\n",
      "Epoch [51/2000], Loss: 0.3197\n",
      "Epoch [52/2000], Loss: 0.1938\n",
      "Epoch [53/2000], Loss: 0.3683\n",
      "Epoch [54/2000], Loss: 0.0951\n",
      "Epoch [55/2000], Loss: 0.1602\n",
      "Epoch [56/2000], Loss: 0.1473\n",
      "Epoch [57/2000], Loss: 0.1408\n",
      "Epoch [58/2000], Loss: 0.1407\n",
      "Epoch [59/2000], Loss: 0.1947\n",
      "Epoch [60/2000], Loss: 0.1771\n",
      "Epoch [61/2000], Loss: 0.1663\n",
      "Epoch [62/2000], Loss: 0.1384\n",
      "Epoch [63/2000], Loss: 0.0533\n",
      "Epoch [64/2000], Loss: 0.0880\n",
      "Epoch [65/2000], Loss: 0.1463\n",
      "Epoch [66/2000], Loss: 0.1517\n",
      "Epoch [67/2000], Loss: 0.0453\n",
      "Epoch [68/2000], Loss: 0.1189\n",
      "Epoch [69/2000], Loss: 0.1133\n",
      "Epoch [70/2000], Loss: 0.0916\n",
      "Epoch [71/2000], Loss: 0.0994\n",
      "Epoch [72/2000], Loss: 0.0710\n",
      "Epoch [73/2000], Loss: 0.0485\n",
      "Epoch [74/2000], Loss: 0.0836\n",
      "Epoch [75/2000], Loss: 0.0764\n",
      "Epoch [76/2000], Loss: 0.0466\n",
      "Epoch [77/2000], Loss: 0.0391\n",
      "Epoch [78/2000], Loss: 0.0618\n",
      "Epoch [79/2000], Loss: 0.0568\n",
      "Epoch [80/2000], Loss: 0.0497\n",
      "Epoch [81/2000], Loss: 0.0600\n",
      "Epoch [82/2000], Loss: 0.0547\n",
      "Epoch [83/2000], Loss: 0.0664\n",
      "Epoch [84/2000], Loss: 0.0273\n",
      "Epoch [85/2000], Loss: 0.0257\n",
      "Epoch [86/2000], Loss: 0.0559\n",
      "Epoch [87/2000], Loss: 0.1504\n",
      "Epoch [88/2000], Loss: 0.0844\n",
      "Epoch [89/2000], Loss: 0.0381\n",
      "Epoch [90/2000], Loss: 0.0166\n",
      "Epoch [91/2000], Loss: 0.0191\n",
      "Epoch [92/2000], Loss: 0.0887\n",
      "Epoch [93/2000], Loss: 0.0247\n",
      "Epoch [94/2000], Loss: 0.0694\n",
      "Epoch [95/2000], Loss: 0.0478\n",
      "Epoch [96/2000], Loss: 0.0244\n",
      "Epoch [97/2000], Loss: 0.0226\n",
      "Epoch [98/2000], Loss: 0.0714\n",
      "Epoch [99/2000], Loss: 0.0329\n",
      "Epoch [100/2000], Loss: 0.0326\n",
      "Epoch [101/2000], Loss: 0.0326\n",
      "Epoch [102/2000], Loss: 0.0204\n",
      "Epoch [103/2000], Loss: 0.0308\n",
      "Epoch [104/2000], Loss: 0.0202\n",
      "Epoch [105/2000], Loss: 0.0124\n",
      "Epoch [106/2000], Loss: 0.0462\n",
      "Epoch [107/2000], Loss: 0.0200\n",
      "Epoch [108/2000], Loss: 0.0333\n",
      "Epoch [109/2000], Loss: 0.0364\n",
      "Epoch [110/2000], Loss: 0.0249\n",
      "Epoch [111/2000], Loss: 0.0222\n",
      "Epoch [112/2000], Loss: 0.0290\n",
      "Epoch [113/2000], Loss: 0.0216\n",
      "Epoch [114/2000], Loss: 0.0251\n",
      "Epoch [115/2000], Loss: 0.0205\n",
      "Epoch [116/2000], Loss: 0.0301\n",
      "Epoch [117/2000], Loss: 0.0135\n",
      "Epoch [118/2000], Loss: 0.0220\n",
      "Epoch [119/2000], Loss: 0.0102\n",
      "Epoch [120/2000], Loss: 0.0170\n",
      "Epoch [121/2000], Loss: 0.0094\n",
      "Epoch [122/2000], Loss: 0.0166\n",
      "Epoch [123/2000], Loss: 0.0091\n",
      "Epoch [124/2000], Loss: 0.0176\n",
      "Epoch [125/2000], Loss: 0.0180\n",
      "Epoch [126/2000], Loss: 0.0218\n",
      "Epoch [127/2000], Loss: 0.0249\n",
      "Epoch [128/2000], Loss: 0.0167\n",
      "Epoch [129/2000], Loss: 0.0076\n",
      "Epoch [130/2000], Loss: 0.0121\n",
      "Epoch [131/2000], Loss: 0.0175\n",
      "Epoch [132/2000], Loss: 0.0126\n",
      "Epoch [133/2000], Loss: 0.0105\n",
      "Epoch [134/2000], Loss: 0.0038\n",
      "Epoch [135/2000], Loss: 0.0132\n",
      "Epoch [136/2000], Loss: 0.0231\n",
      "Epoch [137/2000], Loss: 0.0172\n",
      "Epoch [138/2000], Loss: 0.0116\n",
      "Epoch [139/2000], Loss: 0.0193\n",
      "Epoch [140/2000], Loss: 0.0228\n",
      "Epoch [141/2000], Loss: 0.0101\n",
      "Epoch [142/2000], Loss: 0.0141\n",
      "Epoch [143/2000], Loss: 0.0114\n",
      "Epoch [144/2000], Loss: 0.0123\n",
      "Epoch [145/2000], Loss: 0.0168\n",
      "Epoch [146/2000], Loss: 0.0144\n",
      "Epoch [147/2000], Loss: 0.0103\n",
      "Epoch [148/2000], Loss: 0.0060\n",
      "Epoch [149/2000], Loss: 0.0128\n",
      "Epoch [150/2000], Loss: 0.0151\n",
      "Epoch [151/2000], Loss: 0.0056\n",
      "Epoch [152/2000], Loss: 0.0139\n",
      "Epoch [153/2000], Loss: 0.0132\n",
      "Epoch [154/2000], Loss: 0.0122\n",
      "Epoch [155/2000], Loss: 0.0116\n",
      "Epoch [156/2000], Loss: 0.0077\n",
      "Epoch [157/2000], Loss: 0.0096\n",
      "Epoch [158/2000], Loss: 0.0115\n",
      "Epoch [159/2000], Loss: 0.0072\n",
      "Epoch [160/2000], Loss: 0.0070\n",
      "Epoch [161/2000], Loss: 0.0078\n",
      "Epoch [162/2000], Loss: 0.0106\n",
      "Epoch [163/2000], Loss: 0.0106\n",
      "Epoch [164/2000], Loss: 0.0065\n",
      "Epoch [165/2000], Loss: 0.0080\n",
      "Epoch [166/2000], Loss: 0.0066\n",
      "Epoch [167/2000], Loss: 0.0063\n",
      "Epoch [168/2000], Loss: 0.0118\n",
      "Epoch [169/2000], Loss: 0.0205\n",
      "Epoch [170/2000], Loss: 0.0100\n",
      "Epoch [171/2000], Loss: 0.0076\n",
      "Epoch [172/2000], Loss: 0.0063\n",
      "Epoch [173/2000], Loss: 0.0091\n",
      "Epoch [174/2000], Loss: 0.0125\n",
      "Epoch [175/2000], Loss: 0.0074\n",
      "Epoch [176/2000], Loss: 0.0055\n",
      "Epoch [177/2000], Loss: 0.0052\n",
      "Epoch [178/2000], Loss: 0.0048\n",
      "Epoch [179/2000], Loss: 0.0044\n",
      "Epoch [180/2000], Loss: 0.0040\n",
      "Epoch [181/2000], Loss: 0.0072\n",
      "Epoch [182/2000], Loss: 0.0037\n",
      "Epoch [183/2000], Loss: 0.0084\n",
      "Epoch [184/2000], Loss: 0.0061\n",
      "Epoch [185/2000], Loss: 0.0101\n",
      "Epoch [186/2000], Loss: 0.0044\n",
      "Epoch [187/2000], Loss: 0.0078\n",
      "Epoch [188/2000], Loss: 0.0104\n",
      "Epoch [189/2000], Loss: 0.0043\n",
      "Epoch [190/2000], Loss: 0.0084\n",
      "Epoch [191/2000], Loss: 0.0075\n",
      "Epoch [192/2000], Loss: 0.0025\n",
      "Epoch [193/2000], Loss: 0.0065\n",
      "Epoch [194/2000], Loss: 0.0081\n",
      "Epoch [195/2000], Loss: 0.0042\n",
      "Epoch [196/2000], Loss: 0.0078\n",
      "Epoch [197/2000], Loss: 0.0041\n",
      "Epoch [198/2000], Loss: 0.0027\n",
      "Epoch [199/2000], Loss: 0.0060\n",
      "Epoch [200/2000], Loss: 0.0066\n",
      "Epoch [201/2000], Loss: 0.0058\n",
      "Epoch [202/2000], Loss: 0.0055\n",
      "Epoch [203/2000], Loss: 0.0045\n",
      "Epoch [204/2000], Loss: 0.0038\n",
      "Epoch [205/2000], Loss: 0.0033\n",
      "Epoch [206/2000], Loss: 0.0068\n",
      "Epoch [207/2000], Loss: 0.0052\n",
      "Epoch [208/2000], Loss: 0.0034\n",
      "Epoch [209/2000], Loss: 0.0043\n",
      "Epoch [210/2000], Loss: 0.0015\n",
      "Epoch [211/2000], Loss: 0.0042\n",
      "Epoch [212/2000], Loss: 0.0024\n",
      "Epoch [213/2000], Loss: 0.0032\n",
      "Epoch [214/2000], Loss: 0.0066\n",
      "Epoch [215/2000], Loss: 0.0025\n",
      "Epoch [216/2000], Loss: 0.0117\n",
      "Epoch [217/2000], Loss: 0.0050\n",
      "Epoch [218/2000], Loss: 0.0050\n",
      "Epoch [219/2000], Loss: 0.0062\n",
      "Epoch [220/2000], Loss: 0.0026\n",
      "Epoch [221/2000], Loss: 0.0083\n",
      "Epoch [222/2000], Loss: 0.0024\n",
      "Epoch [223/2000], Loss: 0.0035\n",
      "Epoch [224/2000], Loss: 0.0027\n",
      "Epoch [225/2000], Loss: 0.0058\n",
      "Epoch [226/2000], Loss: 0.0045\n",
      "Epoch [227/2000], Loss: 0.0026\n",
      "Epoch [228/2000], Loss: 0.0032\n",
      "Epoch [229/2000], Loss: 0.0028\n",
      "Epoch [230/2000], Loss: 0.0034\n",
      "Epoch [231/2000], Loss: 0.0032\n",
      "Epoch [232/2000], Loss: 0.0067\n",
      "Epoch [233/2000], Loss: 0.0044\n",
      "Epoch [234/2000], Loss: 0.0046\n",
      "Epoch [235/2000], Loss: 0.0022\n",
      "Epoch [236/2000], Loss: 0.0036\n",
      "Epoch [237/2000], Loss: 0.0034\n",
      "Epoch [238/2000], Loss: 0.0045\n",
      "Epoch [239/2000], Loss: 0.0026\n",
      "Epoch [240/2000], Loss: 0.0041\n",
      "Epoch [241/2000], Loss: 0.0045\n",
      "Epoch [242/2000], Loss: 0.0017\n",
      "Epoch [243/2000], Loss: 0.0038\n",
      "Epoch [244/2000], Loss: 0.0028\n",
      "Epoch [245/2000], Loss: 0.0047\n",
      "Epoch [246/2000], Loss: 0.0020\n",
      "Epoch [247/2000], Loss: 0.0041\n",
      "Epoch [248/2000], Loss: 0.0036\n",
      "Epoch [249/2000], Loss: 0.0045\n",
      "Epoch [250/2000], Loss: 0.0051\n",
      "Epoch [251/2000], Loss: 0.0025\n",
      "Epoch [252/2000], Loss: 0.0018\n",
      "Epoch [253/2000], Loss: 0.0044\n",
      "Epoch [254/2000], Loss: 0.0042\n",
      "Epoch [255/2000], Loss: 0.0022\n",
      "Epoch [256/2000], Loss: 0.0026\n",
      "Epoch [257/2000], Loss: 0.0021\n",
      "Epoch [258/2000], Loss: 0.0025\n",
      "Epoch [259/2000], Loss: 0.0040\n",
      "Epoch [260/2000], Loss: 0.0038\n",
      "Epoch [261/2000], Loss: 0.0033\n",
      "Epoch [262/2000], Loss: 0.0027\n",
      "Epoch [263/2000], Loss: 0.0028\n",
      "Epoch [264/2000], Loss: 0.0017\n",
      "Epoch [265/2000], Loss: 0.0025\n",
      "Epoch [266/2000], Loss: 0.0027\n",
      "Epoch [267/2000], Loss: 0.0024\n",
      "Epoch [268/2000], Loss: 0.0029\n",
      "Epoch [269/2000], Loss: 0.0023\n",
      "Epoch [270/2000], Loss: 0.0020\n",
      "Epoch [271/2000], Loss: 0.0011\n",
      "Epoch [272/2000], Loss: 0.0016\n",
      "Epoch [273/2000], Loss: 0.0028\n",
      "Epoch [274/2000], Loss: 0.0033\n",
      "Epoch [275/2000], Loss: 0.0019\n",
      "Epoch [276/2000], Loss: 0.0022\n",
      "Epoch [277/2000], Loss: 0.0035\n",
      "Epoch [278/2000], Loss: 0.0039\n",
      "Epoch [279/2000], Loss: 0.0024\n",
      "Epoch [280/2000], Loss: 0.0033\n",
      "Epoch [281/2000], Loss: 0.0036\n",
      "Epoch [282/2000], Loss: 0.0023\n",
      "Epoch [283/2000], Loss: 0.0045\n",
      "Epoch [284/2000], Loss: 0.0028\n",
      "Epoch [285/2000], Loss: 0.0023\n",
      "Epoch [286/2000], Loss: 0.0019\n",
      "Epoch [287/2000], Loss: 0.0011\n",
      "Epoch [288/2000], Loss: 0.0036\n",
      "Epoch [289/2000], Loss: 0.0031\n",
      "Epoch [290/2000], Loss: 0.0030\n",
      "Epoch [291/2000], Loss: 0.0024\n",
      "Epoch [292/2000], Loss: 0.0011\n",
      "Epoch [293/2000], Loss: 0.0028\n",
      "Epoch [294/2000], Loss: 0.0022\n",
      "Epoch [295/2000], Loss: 0.0027\n",
      "Epoch [296/2000], Loss: 0.0020\n",
      "Epoch [297/2000], Loss: 0.0020\n",
      "Epoch [298/2000], Loss: 0.0018\n",
      "Epoch [299/2000], Loss: 0.0013\n",
      "Epoch [300/2000], Loss: 0.0035\n",
      "Epoch [301/2000], Loss: 0.0019\n",
      "Epoch [302/2000], Loss: 0.0012\n",
      "Epoch [303/2000], Loss: 0.0021\n",
      "Epoch [304/2000], Loss: 0.0028\n",
      "Epoch [305/2000], Loss: 0.0017\n",
      "Epoch [306/2000], Loss: 0.0028\n",
      "Epoch [307/2000], Loss: 0.0023\n",
      "Epoch [308/2000], Loss: 0.0025\n",
      "Epoch [309/2000], Loss: 0.0020\n",
      "Epoch [310/2000], Loss: 0.0010\n",
      "Epoch [311/2000], Loss: 0.0009\n",
      "Epoch [312/2000], Loss: 0.0016\n",
      "Epoch [313/2000], Loss: 0.0020\n",
      "Epoch [314/2000], Loss: 0.0038\n",
      "Epoch [315/2000], Loss: 0.0009\n",
      "Epoch [316/2000], Loss: 0.0025\n",
      "Epoch [317/2000], Loss: 0.0016\n",
      "Epoch [318/2000], Loss: 0.0025\n",
      "Epoch [319/2000], Loss: 0.0016\n",
      "Epoch [320/2000], Loss: 0.0013\n",
      "Epoch [321/2000], Loss: 0.0021\n",
      "Epoch [322/2000], Loss: 0.0020\n",
      "Epoch [323/2000], Loss: 0.0011\n",
      "Epoch [324/2000], Loss: 0.0016\n",
      "Epoch [325/2000], Loss: 0.0023\n",
      "Epoch [326/2000], Loss: 0.0008\n",
      "Epoch [327/2000], Loss: 0.0018\n",
      "Epoch [328/2000], Loss: 0.0012\n",
      "Epoch [329/2000], Loss: 0.0022\n",
      "Epoch [330/2000], Loss: 0.0023\n",
      "Epoch [331/2000], Loss: 0.0019\n",
      "Epoch [332/2000], Loss: 0.0008\n",
      "Epoch [333/2000], Loss: 0.0014\n",
      "Epoch [334/2000], Loss: 0.0014\n",
      "Epoch [335/2000], Loss: 0.0015\n",
      "Epoch [336/2000], Loss: 0.0022\n",
      "Epoch [337/2000], Loss: 0.0028\n",
      "Epoch [338/2000], Loss: 0.0010\n",
      "Epoch [339/2000], Loss: 0.0014\n",
      "Epoch [340/2000], Loss: 0.0014\n",
      "Epoch [341/2000], Loss: 0.0011\n",
      "Epoch [342/2000], Loss: 0.0022\n",
      "Epoch [343/2000], Loss: 0.0020\n",
      "Epoch [344/2000], Loss: 0.0020\n",
      "Epoch [345/2000], Loss: 0.0019\n",
      "Epoch [346/2000], Loss: 0.0016\n",
      "Epoch [347/2000], Loss: 0.0011\n",
      "Epoch [348/2000], Loss: 0.0015\n",
      "Epoch [349/2000], Loss: 0.0013\n",
      "Epoch [350/2000], Loss: 0.0022\n",
      "Epoch [351/2000], Loss: 0.0008\n",
      "Epoch [352/2000], Loss: 0.0017\n",
      "Epoch [353/2000], Loss: 0.0013\n",
      "Epoch [354/2000], Loss: 0.0014\n",
      "Epoch [355/2000], Loss: 0.0014\n",
      "Epoch [356/2000], Loss: 0.0007\n",
      "Epoch [357/2000], Loss: 0.0020\n",
      "Epoch [358/2000], Loss: 0.0011\n",
      "Epoch [359/2000], Loss: 0.0011\n",
      "Epoch [360/2000], Loss: 0.0022\n",
      "Epoch [361/2000], Loss: 0.0019\n",
      "Epoch [362/2000], Loss: 0.0015\n",
      "Epoch [363/2000], Loss: 0.0021\n",
      "Epoch [364/2000], Loss: 0.0014\n",
      "Epoch [365/2000], Loss: 0.0009\n",
      "Epoch [366/2000], Loss: 0.0026\n",
      "Epoch [367/2000], Loss: 0.0011\n",
      "Epoch [368/2000], Loss: 0.0018\n",
      "Epoch [369/2000], Loss: 0.0019\n",
      "Epoch [370/2000], Loss: 0.0007\n",
      "Epoch [371/2000], Loss: 0.0012\n",
      "Epoch [372/2000], Loss: 0.0018\n",
      "Epoch [373/2000], Loss: 0.0011\n",
      "Epoch [374/2000], Loss: 0.0010\n",
      "Epoch [375/2000], Loss: 0.0015\n",
      "Epoch [376/2000], Loss: 0.0008\n",
      "Epoch [377/2000], Loss: 0.0018\n",
      "Epoch [378/2000], Loss: 0.0009\n",
      "Epoch [379/2000], Loss: 0.0014\n",
      "Epoch [380/2000], Loss: 0.0018\n",
      "Epoch [381/2000], Loss: 0.0015\n",
      "Epoch [382/2000], Loss: 0.0017\n",
      "Epoch [383/2000], Loss: 0.0008\n",
      "Epoch [384/2000], Loss: 0.0013\n",
      "Epoch [385/2000], Loss: 0.0012\n",
      "Epoch [386/2000], Loss: 0.0021\n",
      "Epoch [387/2000], Loss: 0.0011\n",
      "Epoch [388/2000], Loss: 0.0007\n",
      "Epoch [389/2000], Loss: 0.0008\n",
      "Epoch [390/2000], Loss: 0.0023\n",
      "Epoch [391/2000], Loss: 0.0005\n",
      "Epoch [392/2000], Loss: 0.0006\n",
      "Epoch [393/2000], Loss: 0.0016\n",
      "Epoch [394/2000], Loss: 0.0017\n",
      "Epoch [395/2000], Loss: 0.0007\n",
      "Epoch [396/2000], Loss: 0.0013\n",
      "Epoch [397/2000], Loss: 0.0011\n",
      "Epoch [398/2000], Loss: 0.0016\n",
      "Epoch [399/2000], Loss: 0.0008\n",
      "Epoch [400/2000], Loss: 0.0006\n",
      "Epoch [401/2000], Loss: 0.0008\n",
      "Epoch [402/2000], Loss: 0.0008\n",
      "Epoch [403/2000], Loss: 0.0012\n",
      "Epoch [404/2000], Loss: 0.0009\n",
      "Epoch [405/2000], Loss: 0.0010\n",
      "Epoch [406/2000], Loss: 0.0009\n",
      "Epoch [407/2000], Loss: 0.0011\n",
      "Epoch [408/2000], Loss: 0.0012\n",
      "Epoch [409/2000], Loss: 0.0006\n",
      "Epoch [410/2000], Loss: 0.0009\n",
      "Epoch [411/2000], Loss: 0.0006\n",
      "Epoch [412/2000], Loss: 0.0017\n",
      "Epoch [413/2000], Loss: 0.0010\n",
      "Epoch [414/2000], Loss: 0.0012\n",
      "Epoch [415/2000], Loss: 0.0015\n",
      "Epoch [416/2000], Loss: 0.0004\n",
      "Epoch [417/2000], Loss: 0.0006\n",
      "Epoch [418/2000], Loss: 0.0007\n",
      "Epoch [419/2000], Loss: 0.0005\n",
      "Epoch [420/2000], Loss: 0.0011\n",
      "Epoch [421/2000], Loss: 0.0011\n",
      "Epoch [422/2000], Loss: 0.0009\n",
      "Epoch [423/2000], Loss: 0.0007\n",
      "Epoch [424/2000], Loss: 0.0006\n",
      "Epoch [425/2000], Loss: 0.0007\n",
      "Epoch [426/2000], Loss: 0.0003\n",
      "Epoch [427/2000], Loss: 0.0012\n",
      "Epoch [428/2000], Loss: 0.0007\n",
      "Epoch [429/2000], Loss: 0.0009\n",
      "Epoch [430/2000], Loss: 0.0004\n",
      "Epoch [431/2000], Loss: 0.0011\n",
      "Epoch [432/2000], Loss: 0.0015\n",
      "Epoch [433/2000], Loss: 0.0007\n",
      "Epoch [434/2000], Loss: 0.0008\n",
      "Epoch [435/2000], Loss: 0.0009\n",
      "Epoch [436/2000], Loss: 0.0009\n",
      "Epoch [437/2000], Loss: 0.0013\n",
      "Epoch [438/2000], Loss: 0.0012\n",
      "Epoch [439/2000], Loss: 0.0022\n",
      "Epoch [440/2000], Loss: 0.0006\n",
      "Epoch [441/2000], Loss: 0.0008\n",
      "Epoch [442/2000], Loss: 0.0008\n",
      "Epoch [443/2000], Loss: 0.0005\n",
      "Epoch [444/2000], Loss: 0.0010\n",
      "Epoch [445/2000], Loss: 0.0004\n",
      "Epoch [446/2000], Loss: 0.0006\n",
      "Epoch [447/2000], Loss: 0.0009\n",
      "Epoch [448/2000], Loss: 0.0010\n",
      "Epoch [449/2000], Loss: 0.0009\n",
      "Epoch [450/2000], Loss: 0.0011\n",
      "Epoch [451/2000], Loss: 0.0008\n",
      "Epoch [452/2000], Loss: 0.0005\n",
      "Epoch [453/2000], Loss: 0.0009\n",
      "Epoch [454/2000], Loss: 0.0009\n",
      "Epoch [455/2000], Loss: 0.0009\n",
      "Epoch [456/2000], Loss: 0.0009\n",
      "Epoch [457/2000], Loss: 0.0005\n",
      "Epoch [458/2000], Loss: 0.0008\n",
      "Epoch [459/2000], Loss: 0.0006\n",
      "Epoch [460/2000], Loss: 0.0007\n",
      "Epoch [461/2000], Loss: 0.0006\n",
      "Epoch [462/2000], Loss: 0.0007\n",
      "Epoch [463/2000], Loss: 0.0002\n",
      "Epoch [464/2000], Loss: 0.0008\n",
      "Epoch [465/2000], Loss: 0.0006\n",
      "Epoch [466/2000], Loss: 0.0007\n",
      "Epoch [467/2000], Loss: 0.0006\n",
      "Epoch [468/2000], Loss: 0.0010\n",
      "Epoch [469/2000], Loss: 0.0008\n",
      "Epoch [470/2000], Loss: 0.0006\n",
      "Epoch [471/2000], Loss: 0.0010\n",
      "Epoch [472/2000], Loss: 0.0007\n",
      "Epoch [473/2000], Loss: 0.0005\n",
      "Epoch [474/2000], Loss: 0.0003\n",
      "Epoch [475/2000], Loss: 0.0005\n",
      "Epoch [476/2000], Loss: 0.0009\n",
      "Epoch [477/2000], Loss: 0.0007\n",
      "Epoch [478/2000], Loss: 0.0005\n",
      "Epoch [479/2000], Loss: 0.0006\n",
      "Epoch [480/2000], Loss: 0.0004\n",
      "Epoch [481/2000], Loss: 0.0007\n",
      "Epoch [482/2000], Loss: 0.0010\n",
      "Epoch [483/2000], Loss: 0.0009\n",
      "Epoch [484/2000], Loss: 0.0005\n",
      "Epoch [485/2000], Loss: 0.0007\n",
      "Epoch [486/2000], Loss: 0.0003\n",
      "Epoch [487/2000], Loss: 0.0007\n",
      "Epoch [488/2000], Loss: 0.0006\n",
      "Epoch [489/2000], Loss: 0.0009\n",
      "Epoch [490/2000], Loss: 0.0010\n",
      "Epoch [491/2000], Loss: 0.0006\n",
      "Epoch [492/2000], Loss: 0.0008\n",
      "Epoch [493/2000], Loss: 0.0009\n",
      "Epoch [494/2000], Loss: 0.0004\n",
      "Epoch [495/2000], Loss: 0.0003\n",
      "Epoch [496/2000], Loss: 0.0004\n",
      "Epoch [497/2000], Loss: 0.0003\n",
      "Epoch [498/2000], Loss: 0.0005\n",
      "Epoch [499/2000], Loss: 0.0008\n",
      "Epoch [500/2000], Loss: 0.0006\n",
      "Epoch [501/2000], Loss: 0.0009\n",
      "Epoch [502/2000], Loss: 0.0004\n",
      "Epoch [503/2000], Loss: 0.0002\n",
      "Epoch [504/2000], Loss: 0.0003\n",
      "Epoch [505/2000], Loss: 0.0007\n",
      "Epoch [506/2000], Loss: 0.0005\n",
      "Epoch [507/2000], Loss: 0.0005\n",
      "Epoch [508/2000], Loss: 0.0002\n",
      "Epoch [509/2000], Loss: 0.0006\n",
      "Epoch [510/2000], Loss: 0.0006\n",
      "Epoch [511/2000], Loss: 0.0005\n",
      "Epoch [512/2000], Loss: 0.0003\n",
      "Epoch [513/2000], Loss: 0.0005\n",
      "Epoch [514/2000], Loss: 0.0006\n",
      "Epoch [515/2000], Loss: 0.0005\n",
      "Epoch [516/2000], Loss: 0.0006\n",
      "Epoch [517/2000], Loss: 0.0003\n",
      "Epoch [518/2000], Loss: 0.0006\n",
      "Epoch [519/2000], Loss: 0.0005\n",
      "Epoch [520/2000], Loss: 0.0008\n",
      "Epoch [521/2000], Loss: 0.0004\n",
      "Epoch [522/2000], Loss: 0.0008\n",
      "Epoch [523/2000], Loss: 0.0003\n",
      "Epoch [524/2000], Loss: 0.0005\n",
      "Epoch [525/2000], Loss: 0.0006\n",
      "Epoch [526/2000], Loss: 0.0004\n",
      "Epoch [527/2000], Loss: 0.0003\n",
      "Epoch [528/2000], Loss: 0.0003\n",
      "Epoch [529/2000], Loss: 0.0007\n",
      "Epoch [530/2000], Loss: 0.0003\n",
      "Epoch [531/2000], Loss: 0.0005\n",
      "Epoch [532/2000], Loss: 0.0007\n",
      "Epoch [533/2000], Loss: 0.0003\n",
      "Epoch [534/2000], Loss: 0.0005\n",
      "Epoch [535/2000], Loss: 0.0005\n",
      "Epoch [536/2000], Loss: 0.0003\n",
      "Epoch [537/2000], Loss: 0.0004\n",
      "Epoch [538/2000], Loss: 0.0004\n",
      "Epoch [539/2000], Loss: 0.0003\n",
      "Epoch [540/2000], Loss: 0.0005\n",
      "Epoch [541/2000], Loss: 0.0004\n",
      "Epoch [542/2000], Loss: 0.0005\n",
      "Epoch [543/2000], Loss: 0.0006\n",
      "Epoch [544/2000], Loss: 0.0006\n",
      "Epoch [545/2000], Loss: 0.0004\n",
      "Epoch [546/2000], Loss: 0.0004\n",
      "Epoch [547/2000], Loss: 0.0004\n",
      "Epoch [548/2000], Loss: 0.0007\n",
      "Epoch [549/2000], Loss: 0.0005\n",
      "Epoch [550/2000], Loss: 0.0004\n",
      "Epoch [551/2000], Loss: 0.0005\n",
      "Epoch [552/2000], Loss: 0.0003\n",
      "Epoch [553/2000], Loss: 0.0005\n",
      "Epoch [554/2000], Loss: 0.0004\n",
      "Epoch [555/2000], Loss: 0.0002\n",
      "Epoch [556/2000], Loss: 0.0003\n",
      "Epoch [557/2000], Loss: 0.0004\n",
      "Epoch [558/2000], Loss: 0.0004\n",
      "Epoch [559/2000], Loss: 0.0004\n",
      "Epoch [560/2000], Loss: 0.0003\n",
      "Epoch [561/2000], Loss: 0.0004\n",
      "Epoch [562/2000], Loss: 0.0004\n",
      "Epoch [563/2000], Loss: 0.0006\n",
      "Epoch [564/2000], Loss: 0.0004\n",
      "Epoch [565/2000], Loss: 0.0005\n",
      "Epoch [566/2000], Loss: 0.0003\n",
      "Epoch [567/2000], Loss: 0.0004\n",
      "Epoch [568/2000], Loss: 0.0003\n",
      "Epoch [569/2000], Loss: 0.0004\n",
      "Epoch [570/2000], Loss: 0.0003\n",
      "Epoch [571/2000], Loss: 0.0004\n",
      "Epoch [572/2000], Loss: 0.0005\n",
      "Epoch [573/2000], Loss: 0.0003\n",
      "Epoch [574/2000], Loss: 0.0006\n",
      "Epoch [575/2000], Loss: 0.0004\n",
      "Epoch [576/2000], Loss: 0.0005\n",
      "Epoch [577/2000], Loss: 0.0006\n",
      "Epoch [578/2000], Loss: 0.0003\n",
      "Epoch [579/2000], Loss: 0.0005\n",
      "Epoch [580/2000], Loss: 0.0005\n",
      "Epoch [581/2000], Loss: 0.0004\n",
      "Epoch [582/2000], Loss: 0.0004\n",
      "Epoch [583/2000], Loss: 0.0003\n",
      "Epoch [584/2000], Loss: 0.0002\n",
      "Epoch [585/2000], Loss: 0.0007\n",
      "Epoch [586/2000], Loss: 0.0004\n",
      "Epoch [587/2000], Loss: 0.0005\n",
      "Epoch [588/2000], Loss: 0.0004\n",
      "Epoch [589/2000], Loss: 0.0004\n",
      "Epoch [590/2000], Loss: 0.0007\n",
      "Epoch [591/2000], Loss: 0.0004\n",
      "Epoch [592/2000], Loss: 0.0004\n",
      "Epoch [593/2000], Loss: 0.0004\n",
      "Epoch [594/2000], Loss: 0.0005\n",
      "Epoch [595/2000], Loss: 0.0006\n",
      "Epoch [596/2000], Loss: 0.0005\n",
      "Epoch [597/2000], Loss: 0.0003\n",
      "Epoch [598/2000], Loss: 0.0004\n",
      "Epoch [599/2000], Loss: 0.0004\n",
      "Epoch [600/2000], Loss: 0.0004\n",
      "Epoch [601/2000], Loss: 0.0005\n",
      "Epoch [602/2000], Loss: 0.0004\n",
      "Epoch [603/2000], Loss: 0.0004\n",
      "Epoch [604/2000], Loss: 0.0003\n",
      "Epoch [605/2000], Loss: 0.0004\n",
      "Epoch [606/2000], Loss: 0.0004\n",
      "Epoch [607/2000], Loss: 0.0003\n",
      "Epoch [608/2000], Loss: 0.0005\n",
      "Epoch [609/2000], Loss: 0.0002\n",
      "Epoch [610/2000], Loss: 0.0002\n",
      "Epoch [611/2000], Loss: 0.0006\n",
      "Epoch [612/2000], Loss: 0.0004\n",
      "Epoch [613/2000], Loss: 0.0003\n",
      "Epoch [614/2000], Loss: 0.0003\n",
      "Epoch [615/2000], Loss: 0.0002\n",
      "Epoch [616/2000], Loss: 0.0004\n",
      "Epoch [617/2000], Loss: 0.0003\n",
      "Epoch [618/2000], Loss: 0.0004\n",
      "Epoch [619/2000], Loss: 0.0004\n",
      "Epoch [620/2000], Loss: 0.0002\n",
      "Epoch [621/2000], Loss: 0.0002\n",
      "Epoch [622/2000], Loss: 0.0004\n",
      "Epoch [623/2000], Loss: 0.0004\n",
      "Epoch [624/2000], Loss: 0.0002\n",
      "Epoch [625/2000], Loss: 0.0003\n",
      "Epoch [626/2000], Loss: 0.0005\n",
      "Epoch [627/2000], Loss: 0.0002\n",
      "Epoch [628/2000], Loss: 0.0003\n",
      "Epoch [629/2000], Loss: 0.0004\n",
      "Epoch [630/2000], Loss: 0.0003\n",
      "Epoch [631/2000], Loss: 0.0002\n",
      "Epoch [632/2000], Loss: 0.0005\n",
      "Epoch [633/2000], Loss: 0.0003\n",
      "Epoch [634/2000], Loss: 0.0003\n",
      "Epoch [635/2000], Loss: 0.0003\n",
      "Epoch [636/2000], Loss: 0.0002\n",
      "Epoch [637/2000], Loss: 0.0006\n",
      "Epoch [638/2000], Loss: 0.0002\n",
      "Epoch [639/2000], Loss: 0.0003\n",
      "Epoch [640/2000], Loss: 0.0003\n",
      "Epoch [641/2000], Loss: 0.0004\n",
      "Epoch [642/2000], Loss: 0.0002\n",
      "Epoch [643/2000], Loss: 0.0002\n",
      "Epoch [644/2000], Loss: 0.0002\n",
      "Epoch [645/2000], Loss: 0.0003\n",
      "Epoch [646/2000], Loss: 0.0002\n",
      "Epoch [647/2000], Loss: 0.0004\n",
      "Epoch [648/2000], Loss: 0.0002\n",
      "Epoch [649/2000], Loss: 0.0004\n",
      "Epoch [650/2000], Loss: 0.0003\n",
      "Epoch [651/2000], Loss: 0.0005\n",
      "Epoch [652/2000], Loss: 0.0004\n",
      "Epoch [653/2000], Loss: 0.0004\n",
      "Epoch [654/2000], Loss: 0.0003\n",
      "Epoch [655/2000], Loss: 0.0003\n",
      "Epoch [656/2000], Loss: 0.0002\n",
      "Epoch [657/2000], Loss: 0.0002\n",
      "Epoch [658/2000], Loss: 0.0002\n",
      "Epoch [659/2000], Loss: 0.0004\n",
      "Epoch [660/2000], Loss: 0.0003\n",
      "Epoch [661/2000], Loss: 0.0004\n",
      "Epoch [662/2000], Loss: 0.0003\n",
      "Epoch [663/2000], Loss: 0.0002\n",
      "Epoch [664/2000], Loss: 0.0002\n",
      "Epoch [665/2000], Loss: 0.0004\n",
      "Epoch [666/2000], Loss: 0.0001\n",
      "Epoch [667/2000], Loss: 0.0002\n",
      "Epoch [668/2000], Loss: 0.0002\n",
      "Epoch [669/2000], Loss: 0.0004\n",
      "Epoch [670/2000], Loss: 0.0003\n",
      "Epoch [671/2000], Loss: 0.0003\n",
      "Epoch [672/2000], Loss: 0.0001\n",
      "Epoch [673/2000], Loss: 0.0003\n",
      "Epoch [674/2000], Loss: 0.0002\n",
      "Epoch [675/2000], Loss: 0.0003\n",
      "Epoch [676/2000], Loss: 0.0003\n",
      "Epoch [677/2000], Loss: 0.0002\n",
      "Epoch [678/2000], Loss: 0.0002\n",
      "Epoch [679/2000], Loss: 0.0001\n",
      "Epoch [680/2000], Loss: 0.0003\n",
      "Epoch [681/2000], Loss: 0.0001\n",
      "Epoch [682/2000], Loss: 0.0002\n",
      "Epoch [683/2000], Loss: 0.0003\n",
      "Epoch [684/2000], Loss: 0.0003\n",
      "Epoch [685/2000], Loss: 0.0001\n",
      "Epoch [686/2000], Loss: 0.0002\n",
      "Epoch [687/2000], Loss: 0.0002\n",
      "Epoch [688/2000], Loss: 0.0003\n",
      "Epoch [689/2000], Loss: 0.0003\n",
      "Epoch [690/2000], Loss: 0.0001\n",
      "Epoch [691/2000], Loss: 0.0001\n",
      "Epoch [692/2000], Loss: 0.0003\n",
      "Epoch [693/2000], Loss: 0.0001\n",
      "Epoch [694/2000], Loss: 0.0002\n",
      "Epoch [695/2000], Loss: 0.0003\n",
      "Epoch [696/2000], Loss: 0.0002\n",
      "Epoch [697/2000], Loss: 0.0002\n",
      "Epoch [698/2000], Loss: 0.0003\n",
      "Epoch [699/2000], Loss: 0.0002\n",
      "Epoch [700/2000], Loss: 0.0003\n",
      "Epoch [701/2000], Loss: 0.0001\n",
      "Epoch [702/2000], Loss: 0.0003\n",
      "Epoch [703/2000], Loss: 0.0003\n",
      "Epoch [704/2000], Loss: 0.0001\n",
      "Epoch [705/2000], Loss: 0.0002\n",
      "Epoch [706/2000], Loss: 0.0003\n",
      "Epoch [707/2000], Loss: 0.0003\n",
      "Epoch [708/2000], Loss: 0.0001\n",
      "Epoch [709/2000], Loss: 0.0003\n",
      "Epoch [710/2000], Loss: 0.0002\n",
      "Epoch [711/2000], Loss: 0.0001\n",
      "Epoch [712/2000], Loss: 0.0003\n",
      "Epoch [713/2000], Loss: 0.0002\n",
      "Epoch [714/2000], Loss: 0.0002\n",
      "Epoch [715/2000], Loss: 0.0003\n",
      "Epoch [716/2000], Loss: 0.0004\n",
      "Epoch [717/2000], Loss: 0.0002\n",
      "Epoch [718/2000], Loss: 0.0003\n",
      "Epoch [719/2000], Loss: 0.0002\n",
      "Epoch [720/2000], Loss: 0.0003\n",
      "Epoch [721/2000], Loss: 0.0003\n",
      "Epoch [722/2000], Loss: 0.0002\n",
      "Epoch [723/2000], Loss: 0.0003\n",
      "Epoch [724/2000], Loss: 0.0001\n",
      "Epoch [725/2000], Loss: 0.0003\n",
      "Epoch [726/2000], Loss: 0.0002\n",
      "Epoch [727/2000], Loss: 0.0003\n",
      "Epoch [728/2000], Loss: 0.0002\n",
      "Epoch [729/2000], Loss: 0.0002\n",
      "Epoch [730/2000], Loss: 0.0002\n",
      "Epoch [731/2000], Loss: 0.0002\n",
      "Epoch [732/2000], Loss: 0.0002\n",
      "Epoch [733/2000], Loss: 0.0002\n",
      "Epoch [734/2000], Loss: 0.0002\n",
      "Epoch [735/2000], Loss: 0.0001\n",
      "Epoch [736/2000], Loss: 0.0002\n",
      "Epoch [737/2000], Loss: 0.0003\n",
      "Epoch [738/2000], Loss: 0.0002\n",
      "Epoch [739/2000], Loss: 0.0001\n",
      "Epoch [740/2000], Loss: 0.0002\n",
      "Epoch [741/2000], Loss: 0.0001\n",
      "Epoch [742/2000], Loss: 0.0001\n",
      "Epoch [743/2000], Loss: 0.0002\n",
      "Epoch [744/2000], Loss: 0.0003\n",
      "Epoch [745/2000], Loss: 0.0001\n",
      "Epoch [746/2000], Loss: 0.0002\n",
      "Epoch [747/2000], Loss: 0.0002\n",
      "Epoch [748/2000], Loss: 0.0003\n",
      "Epoch [749/2000], Loss: 0.0002\n",
      "Epoch [750/2000], Loss: 0.0001\n",
      "Epoch [751/2000], Loss: 0.0001\n",
      "Epoch [752/2000], Loss: 0.0003\n",
      "Epoch [753/2000], Loss: 0.0002\n",
      "Epoch [754/2000], Loss: 0.0002\n",
      "Epoch [755/2000], Loss: 0.0001\n",
      "Epoch [756/2000], Loss: 0.0002\n",
      "Epoch [757/2000], Loss: 0.0002\n",
      "Epoch [758/2000], Loss: 0.0002\n",
      "Epoch [759/2000], Loss: 0.0002\n",
      "Epoch [760/2000], Loss: 0.0000\n",
      "Epoch [761/2000], Loss: 0.0002\n",
      "Epoch [762/2000], Loss: 0.0002\n",
      "Epoch [763/2000], Loss: 0.0002\n",
      "Epoch [764/2000], Loss: 0.0001\n",
      "Epoch [765/2000], Loss: 0.0001\n",
      "Epoch [766/2000], Loss: 0.0002\n",
      "Epoch [767/2000], Loss: 0.0001\n",
      "Epoch [768/2000], Loss: 0.0003\n",
      "Epoch [769/2000], Loss: 0.0003\n",
      "Epoch [770/2000], Loss: 0.0002\n",
      "Epoch [771/2000], Loss: 0.0001\n",
      "Epoch [772/2000], Loss: 0.0001\n",
      "Epoch [773/2000], Loss: 0.0001\n",
      "Epoch [774/2000], Loss: 0.0001\n",
      "Epoch [775/2000], Loss: 0.0002\n",
      "Epoch [776/2000], Loss: 0.0001\n",
      "Epoch [777/2000], Loss: 0.0002\n",
      "Epoch [778/2000], Loss: 0.0001\n",
      "Epoch [779/2000], Loss: 0.0001\n",
      "Epoch [780/2000], Loss: 0.0003\n",
      "Epoch [781/2000], Loss: 0.0002\n",
      "Epoch [782/2000], Loss: 0.0002\n",
      "Epoch [783/2000], Loss: 0.0000\n",
      "Epoch [784/2000], Loss: 0.0001\n",
      "Epoch [785/2000], Loss: 0.0002\n",
      "Epoch [786/2000], Loss: 0.0001\n",
      "Epoch [787/2000], Loss: 0.0001\n",
      "Epoch [788/2000], Loss: 0.0002\n",
      "Epoch [789/2000], Loss: 0.0001\n",
      "Epoch [790/2000], Loss: 0.0002\n",
      "Epoch [791/2000], Loss: 0.0001\n",
      "Epoch [792/2000], Loss: 0.0001\n",
      "Epoch [793/2000], Loss: 0.0002\n",
      "Epoch [794/2000], Loss: 0.0001\n",
      "Epoch [795/2000], Loss: 0.0002\n",
      "Epoch [796/2000], Loss: 0.0001\n",
      "Epoch [797/2000], Loss: 0.0002\n",
      "Epoch [798/2000], Loss: 0.0002\n",
      "Epoch [799/2000], Loss: 0.0003\n",
      "Epoch [800/2000], Loss: 0.0002\n",
      "Epoch [801/2000], Loss: 0.0002\n",
      "Epoch [802/2000], Loss: 0.0003\n",
      "Epoch [803/2000], Loss: 0.0001\n",
      "Epoch [804/2000], Loss: 0.0002\n",
      "Epoch [805/2000], Loss: 0.0003\n",
      "Epoch [806/2000], Loss: 0.0002\n",
      "Epoch [807/2000], Loss: 0.0001\n",
      "Epoch [808/2000], Loss: 0.0002\n",
      "Epoch [809/2000], Loss: 0.0002\n",
      "Epoch [810/2000], Loss: 0.0002\n",
      "Epoch [811/2000], Loss: 0.0002\n",
      "Epoch [812/2000], Loss: 0.0001\n",
      "Epoch [813/2000], Loss: 0.0001\n",
      "Epoch [814/2000], Loss: 0.0001\n",
      "Epoch [815/2000], Loss: 0.0002\n",
      "Epoch [816/2000], Loss: 0.0002\n",
      "Epoch [817/2000], Loss: 0.0002\n",
      "Epoch [818/2000], Loss: 0.0002\n",
      "Epoch [819/2000], Loss: 0.0002\n",
      "Epoch [820/2000], Loss: 0.0003\n",
      "Epoch [821/2000], Loss: 0.0001\n",
      "Epoch [822/2000], Loss: 0.0002\n",
      "Epoch [823/2000], Loss: 0.0002\n",
      "Epoch [824/2000], Loss: 0.0002\n",
      "Epoch [825/2000], Loss: 0.0001\n",
      "Epoch [826/2000], Loss: 0.0001\n",
      "Epoch [827/2000], Loss: 0.0002\n",
      "Epoch [828/2000], Loss: 0.0001\n",
      "Epoch [829/2000], Loss: 0.0001\n",
      "Epoch [830/2000], Loss: 0.0002\n",
      "Epoch [831/2000], Loss: 0.0001\n",
      "Epoch [832/2000], Loss: 0.0001\n",
      "Epoch [833/2000], Loss: 0.0001\n",
      "Epoch [834/2000], Loss: 0.0002\n",
      "Epoch [835/2000], Loss: 0.0001\n",
      "Epoch [836/2000], Loss: 0.0002\n",
      "Epoch [837/2000], Loss: 0.0002\n",
      "Epoch [838/2000], Loss: 0.0001\n",
      "Epoch [839/2000], Loss: 0.0001\n",
      "Epoch [840/2000], Loss: 0.0001\n",
      "Epoch [841/2000], Loss: 0.0001\n",
      "Epoch [842/2000], Loss: 0.0001\n",
      "Epoch [843/2000], Loss: 0.0002\n",
      "Epoch [844/2000], Loss: 0.0002\n",
      "Epoch [845/2000], Loss: 0.0001\n",
      "Epoch [846/2000], Loss: 0.0001\n",
      "Epoch [847/2000], Loss: 0.0001\n",
      "Epoch [848/2000], Loss: 0.0001\n",
      "Epoch [849/2000], Loss: 0.0001\n",
      "Epoch [850/2000], Loss: 0.0002\n",
      "Epoch [851/2000], Loss: 0.0001\n",
      "Epoch [852/2000], Loss: 0.0001\n",
      "Epoch [853/2000], Loss: 0.0001\n",
      "Epoch [854/2000], Loss: 0.0001\n",
      "Epoch [855/2000], Loss: 0.0001\n",
      "Epoch [856/2000], Loss: 0.0001\n",
      "Epoch [857/2000], Loss: 0.0002\n",
      "Epoch [858/2000], Loss: 0.0001\n",
      "Epoch [859/2000], Loss: 0.0002\n",
      "Epoch [860/2000], Loss: 0.0001\n",
      "Epoch [861/2000], Loss: 0.0001\n",
      "Epoch [862/2000], Loss: 0.0001\n",
      "Epoch [863/2000], Loss: 0.0001\n",
      "Epoch [864/2000], Loss: 0.0001\n",
      "Epoch [865/2000], Loss: 0.0001\n",
      "Epoch [866/2000], Loss: 0.0002\n",
      "Epoch [867/2000], Loss: 0.0001\n",
      "Epoch [868/2000], Loss: 0.0002\n",
      "Epoch [869/2000], Loss: 0.0001\n",
      "Epoch [870/2000], Loss: 0.0001\n",
      "Epoch [871/2000], Loss: 0.0002\n",
      "Epoch [872/2000], Loss: 0.0002\n",
      "Epoch [873/2000], Loss: 0.0001\n",
      "Epoch [874/2000], Loss: 0.0001\n",
      "Epoch [875/2000], Loss: 0.0001\n",
      "Epoch [876/2000], Loss: 0.0001\n",
      "Epoch [877/2000], Loss: 0.0001\n",
      "Epoch [878/2000], Loss: 0.0001\n",
      "Epoch [879/2000], Loss: 0.0002\n",
      "Epoch [880/2000], Loss: 0.0001\n",
      "Epoch [881/2000], Loss: 0.0001\n",
      "Epoch [882/2000], Loss: 0.0002\n",
      "Epoch [883/2000], Loss: 0.0002\n",
      "Epoch [884/2000], Loss: 0.0002\n",
      "Epoch [885/2000], Loss: 0.0001\n",
      "Epoch [886/2000], Loss: 0.0001\n",
      "Epoch [887/2000], Loss: 0.0001\n",
      "Epoch [888/2000], Loss: 0.0001\n",
      "Epoch [889/2000], Loss: 0.0001\n",
      "Epoch [890/2000], Loss: 0.0001\n",
      "Epoch [891/2000], Loss: 0.0001\n",
      "Epoch [892/2000], Loss: 0.0001\n",
      "Epoch [893/2000], Loss: 0.0001\n",
      "Epoch [894/2000], Loss: 0.0002\n",
      "Epoch [895/2000], Loss: 0.0001\n",
      "Epoch [896/2000], Loss: 0.0001\n",
      "Epoch [897/2000], Loss: 0.0001\n",
      "Epoch [898/2000], Loss: 0.0002\n",
      "Epoch [899/2000], Loss: 0.0001\n",
      "Epoch [900/2000], Loss: 0.0001\n",
      "Epoch [901/2000], Loss: 0.0001\n",
      "Epoch [902/2000], Loss: 0.0001\n",
      "Epoch [903/2000], Loss: 0.0001\n",
      "Epoch [904/2000], Loss: 0.0001\n",
      "Epoch [905/2000], Loss: 0.0001\n",
      "Epoch [906/2000], Loss: 0.0001\n",
      "Epoch [907/2000], Loss: 0.0002\n",
      "Epoch [908/2000], Loss: 0.0001\n",
      "Epoch [909/2000], Loss: 0.0001\n",
      "Epoch [910/2000], Loss: 0.0001\n",
      "Epoch [911/2000], Loss: 0.0001\n",
      "Epoch [912/2000], Loss: 0.0001\n",
      "Epoch [913/2000], Loss: 0.0000\n",
      "Epoch [914/2000], Loss: 0.0001\n",
      "Epoch [915/2000], Loss: 0.0001\n",
      "Epoch [916/2000], Loss: 0.0001\n",
      "Epoch [917/2000], Loss: 0.0001\n",
      "Epoch [918/2000], Loss: 0.0001\n",
      "Epoch [919/2000], Loss: 0.0001\n",
      "Epoch [920/2000], Loss: 0.0001\n",
      "Epoch [921/2000], Loss: 0.0001\n",
      "Epoch [922/2000], Loss: 0.0001\n",
      "Epoch [923/2000], Loss: 0.0001\n",
      "Epoch [924/2000], Loss: 0.0001\n",
      "Epoch [925/2000], Loss: 0.0001\n",
      "Epoch [926/2000], Loss: 0.0001\n",
      "Epoch [927/2000], Loss: 0.0001\n",
      "Epoch [928/2000], Loss: 0.0001\n",
      "Epoch [929/2000], Loss: 0.0001\n",
      "Epoch [930/2000], Loss: 0.0001\n",
      "Epoch [931/2000], Loss: 0.0001\n",
      "Epoch [932/2000], Loss: 0.0001\n",
      "Epoch [933/2000], Loss: 0.0001\n",
      "Epoch [934/2000], Loss: 0.0001\n",
      "Epoch [935/2000], Loss: 0.0001\n",
      "Epoch [936/2000], Loss: 0.0001\n",
      "Epoch [937/2000], Loss: 0.0001\n",
      "Epoch [938/2000], Loss: 0.0001\n",
      "Epoch [939/2000], Loss: 0.0001\n",
      "Epoch [940/2000], Loss: 0.0001\n",
      "Epoch [941/2000], Loss: 0.0001\n",
      "Epoch [942/2000], Loss: 0.0000\n",
      "Epoch [943/2000], Loss: 0.0001\n",
      "Epoch [944/2000], Loss: 0.0001\n",
      "Epoch [945/2000], Loss: 0.0001\n",
      "Epoch [946/2000], Loss: 0.0001\n",
      "Epoch [947/2000], Loss: 0.0001\n",
      "Epoch [948/2000], Loss: 0.0001\n",
      "Epoch [949/2000], Loss: 0.0001\n",
      "Epoch [950/2000], Loss: 0.0000\n",
      "Epoch [951/2000], Loss: 0.0001\n",
      "Epoch [952/2000], Loss: 0.0001\n",
      "Epoch [953/2000], Loss: 0.0001\n",
      "Epoch [954/2000], Loss: 0.0001\n",
      "Epoch [955/2000], Loss: 0.0000\n",
      "Epoch [956/2000], Loss: 0.0001\n",
      "Epoch [957/2000], Loss: 0.0001\n",
      "Epoch [958/2000], Loss: 0.0001\n",
      "Epoch [959/2000], Loss: 0.0001\n",
      "Epoch [960/2000], Loss: 0.0001\n",
      "Epoch [961/2000], Loss: 0.0001\n",
      "Epoch [962/2000], Loss: 0.0001\n",
      "Epoch [963/2000], Loss: 0.0001\n",
      "Epoch [964/2000], Loss: 0.0001\n",
      "Epoch [965/2000], Loss: 0.0001\n",
      "Epoch [966/2000], Loss: 0.0001\n",
      "Epoch [967/2000], Loss: 0.0001\n",
      "Epoch [968/2000], Loss: 0.0001\n",
      "Epoch [969/2000], Loss: 0.0001\n",
      "Epoch [970/2000], Loss: 0.0001\n",
      "Epoch [971/2000], Loss: 0.0001\n",
      "Epoch [972/2000], Loss: 0.0001\n",
      "Epoch [973/2000], Loss: 0.0001\n",
      "Epoch [974/2000], Loss: 0.0000\n",
      "Epoch [975/2000], Loss: 0.0001\n",
      "Epoch [976/2000], Loss: 0.0001\n",
      "Epoch [977/2000], Loss: 0.0001\n",
      "Epoch [978/2000], Loss: 0.0001\n",
      "Epoch [979/2000], Loss: 0.0000\n",
      "Epoch [980/2000], Loss: 0.0000\n",
      "Epoch [981/2000], Loss: 0.0001\n",
      "Epoch [982/2000], Loss: 0.0001\n",
      "Epoch [983/2000], Loss: 0.0000\n",
      "Epoch [984/2000], Loss: 0.0001\n",
      "Epoch [985/2000], Loss: 0.0001\n",
      "Epoch [986/2000], Loss: 0.0001\n",
      "Epoch [987/2000], Loss: 0.0000\n",
      "Epoch [988/2000], Loss: 0.0001\n",
      "Epoch [989/2000], Loss: 0.0001\n",
      "Epoch [990/2000], Loss: 0.0001\n",
      "Epoch [991/2000], Loss: 0.0000\n",
      "Epoch [992/2000], Loss: 0.0001\n",
      "Epoch [993/2000], Loss: 0.0001\n",
      "Epoch [994/2000], Loss: 0.0001\n",
      "Epoch [995/2000], Loss: 0.0001\n",
      "Epoch [996/2000], Loss: 0.0001\n",
      "Epoch [997/2000], Loss: 0.0001\n",
      "Epoch [998/2000], Loss: 0.0001\n",
      "Epoch [999/2000], Loss: 0.0001\n",
      "Epoch [1000/2000], Loss: 0.0001\n",
      "Epoch [1001/2000], Loss: 0.0001\n",
      "Epoch [1002/2000], Loss: 0.0001\n",
      "Epoch [1003/2000], Loss: 0.0001\n",
      "Epoch [1004/2000], Loss: 0.0001\n",
      "Epoch [1005/2000], Loss: 0.0000\n",
      "Epoch [1006/2000], Loss: 0.0000\n",
      "Epoch [1007/2000], Loss: 0.0001\n",
      "Epoch [1008/2000], Loss: 0.0000\n",
      "Epoch [1009/2000], Loss: 0.0001\n",
      "Epoch [1010/2000], Loss: 0.0000\n",
      "Epoch [1011/2000], Loss: 0.0001\n",
      "Epoch [1012/2000], Loss: 0.0001\n",
      "Epoch [1013/2000], Loss: 0.0001\n",
      "Epoch [1014/2000], Loss: 0.0000\n",
      "Epoch [1015/2000], Loss: 0.0000\n",
      "Epoch [1016/2000], Loss: 0.0001\n",
      "Epoch [1017/2000], Loss: 0.0001\n",
      "Epoch [1018/2000], Loss: 0.0001\n",
      "Epoch [1019/2000], Loss: 0.0001\n",
      "Epoch [1020/2000], Loss: 0.0001\n",
      "Epoch [1021/2000], Loss: 0.0000\n",
      "Epoch [1022/2000], Loss: 0.0001\n",
      "Epoch [1023/2000], Loss: 0.0001\n",
      "Epoch [1024/2000], Loss: 0.0001\n",
      "Epoch [1025/2000], Loss: 0.0001\n",
      "Epoch [1026/2000], Loss: 0.0000\n",
      "Epoch [1027/2000], Loss: 0.0001\n",
      "Epoch [1028/2000], Loss: 0.0001\n",
      "Epoch [1029/2000], Loss: 0.0000\n",
      "Epoch [1030/2000], Loss: 0.0001\n",
      "Epoch [1031/2000], Loss: 0.0001\n",
      "Epoch [1032/2000], Loss: 0.0000\n",
      "Epoch [1033/2000], Loss: 0.0001\n",
      "Epoch [1034/2000], Loss: 0.0000\n",
      "Epoch [1035/2000], Loss: 0.0001\n",
      "Epoch [1036/2000], Loss: 0.0001\n",
      "Epoch [1037/2000], Loss: 0.0000\n",
      "Epoch [1038/2000], Loss: 0.0001\n",
      "Epoch [1039/2000], Loss: 0.0001\n",
      "Epoch [1040/2000], Loss: 0.0001\n",
      "Epoch [1041/2000], Loss: 0.0001\n",
      "Epoch [1042/2000], Loss: 0.0001\n",
      "Epoch [1043/2000], Loss: 0.0001\n",
      "Epoch [1044/2000], Loss: 0.0001\n",
      "Epoch [1045/2000], Loss: 0.0000\n",
      "Epoch [1046/2000], Loss: 0.0001\n",
      "Epoch [1047/2000], Loss: 0.0001\n",
      "Epoch [1048/2000], Loss: 0.0001\n",
      "Epoch [1049/2000], Loss: 0.0001\n",
      "Epoch [1050/2000], Loss: 0.0001\n",
      "Epoch [1051/2000], Loss: 0.0000\n",
      "Epoch [1052/2000], Loss: 0.0001\n",
      "Epoch [1053/2000], Loss: 0.0001\n",
      "Epoch [1054/2000], Loss: 0.0000\n",
      "Epoch [1055/2000], Loss: 0.0001\n",
      "Epoch [1056/2000], Loss: 0.0001\n",
      "Epoch [1057/2000], Loss: 0.0001\n",
      "Epoch [1058/2000], Loss: 0.0000\n",
      "Epoch [1059/2000], Loss: 0.0000\n",
      "Epoch [1060/2000], Loss: 0.0001\n",
      "Epoch [1061/2000], Loss: 0.0001\n",
      "Epoch [1062/2000], Loss: 0.0001\n",
      "Epoch [1063/2000], Loss: 0.0000\n",
      "Epoch [1064/2000], Loss: 0.0001\n",
      "Epoch [1065/2000], Loss: 0.0001\n",
      "Epoch [1066/2000], Loss: 0.0001\n",
      "Epoch [1067/2000], Loss: 0.0000\n",
      "Epoch [1068/2000], Loss: 0.0000\n",
      "Epoch [1069/2000], Loss: 0.0000\n",
      "Epoch [1070/2000], Loss: 0.0001\n",
      "Epoch [1071/2000], Loss: 0.0000\n",
      "Epoch [1072/2000], Loss: 0.0001\n",
      "Epoch [1073/2000], Loss: 0.0001\n",
      "Epoch [1074/2000], Loss: 0.0001\n",
      "Epoch [1075/2000], Loss: 0.0001\n",
      "Epoch [1076/2000], Loss: 0.0000\n",
      "Epoch [1077/2000], Loss: 0.0000\n",
      "Epoch [1078/2000], Loss: 0.0000\n",
      "Epoch [1079/2000], Loss: 0.0000\n",
      "Epoch [1080/2000], Loss: 0.0000\n",
      "Epoch [1081/2000], Loss: 0.0000\n",
      "Epoch [1082/2000], Loss: 0.0001\n",
      "Epoch [1083/2000], Loss: 0.0001\n",
      "Epoch [1084/2000], Loss: 0.0000\n",
      "Epoch [1085/2000], Loss: 0.0001\n",
      "Epoch [1086/2000], Loss: 0.0001\n",
      "Epoch [1087/2000], Loss: 0.0000\n",
      "Epoch [1088/2000], Loss: 0.0000\n",
      "Epoch [1089/2000], Loss: 0.0000\n",
      "Epoch [1090/2000], Loss: 0.0000\n",
      "Epoch [1091/2000], Loss: 0.0000\n",
      "Epoch [1092/2000], Loss: 0.0000\n",
      "Epoch [1093/2000], Loss: 0.0000\n",
      "Epoch [1094/2000], Loss: 0.0000\n",
      "Epoch [1095/2000], Loss: 0.0001\n",
      "Epoch [1096/2000], Loss: 0.0001\n",
      "Epoch [1097/2000], Loss: 0.0000\n",
      "Epoch [1098/2000], Loss: 0.0001\n",
      "Epoch [1099/2000], Loss: 0.0001\n",
      "Epoch [1100/2000], Loss: 0.0000\n",
      "Epoch [1101/2000], Loss: 0.0001\n",
      "Epoch [1102/2000], Loss: 0.0000\n",
      "Epoch [1103/2000], Loss: 0.0001\n",
      "Epoch [1104/2000], Loss: 0.0000\n",
      "Epoch [1105/2000], Loss: 0.0001\n",
      "Epoch [1106/2000], Loss: 0.0000\n",
      "Epoch [1107/2000], Loss: 0.0001\n",
      "Epoch [1108/2000], Loss: 0.0000\n",
      "Epoch [1109/2000], Loss: 0.0000\n",
      "Epoch [1110/2000], Loss: 0.0001\n",
      "Epoch [1111/2000], Loss: 0.0000\n",
      "Epoch [1112/2000], Loss: 0.0000\n",
      "Epoch [1113/2000], Loss: 0.0001\n",
      "Epoch [1114/2000], Loss: 0.0000\n",
      "Epoch [1115/2000], Loss: 0.0000\n",
      "Epoch [1116/2000], Loss: 0.0001\n",
      "Epoch [1117/2000], Loss: 0.0001\n",
      "Epoch [1118/2000], Loss: 0.0001\n",
      "Epoch [1119/2000], Loss: 0.0000\n",
      "Epoch [1120/2000], Loss: 0.0001\n",
      "Epoch [1121/2000], Loss: 0.0001\n",
      "Epoch [1122/2000], Loss: 0.0001\n",
      "Epoch [1123/2000], Loss: 0.0000\n",
      "Epoch [1124/2000], Loss: 0.0000\n",
      "Epoch [1125/2000], Loss: 0.0000\n",
      "Epoch [1126/2000], Loss: 0.0000\n",
      "Epoch [1127/2000], Loss: 0.0001\n",
      "Epoch [1128/2000], Loss: 0.0000\n",
      "Epoch [1129/2000], Loss: 0.0000\n",
      "Epoch [1130/2000], Loss: 0.0000\n",
      "Epoch [1131/2000], Loss: 0.0001\n",
      "Epoch [1132/2000], Loss: 0.0000\n",
      "Epoch [1133/2000], Loss: 0.0000\n",
      "Epoch [1134/2000], Loss: 0.0000\n",
      "Epoch [1135/2000], Loss: 0.0001\n",
      "Epoch [1136/2000], Loss: 0.0000\n",
      "Epoch [1137/2000], Loss: 0.0000\n",
      "Epoch [1138/2000], Loss: 0.0001\n",
      "Epoch [1139/2000], Loss: 0.0000\n",
      "Epoch [1140/2000], Loss: 0.0001\n",
      "Epoch [1141/2000], Loss: 0.0001\n",
      "Epoch [1142/2000], Loss: 0.0000\n",
      "Epoch [1143/2000], Loss: 0.0001\n",
      "Epoch [1144/2000], Loss: 0.0000\n",
      "Epoch [1145/2000], Loss: 0.0000\n",
      "Epoch [1146/2000], Loss: 0.0000\n",
      "Epoch [1147/2000], Loss: 0.0000\n",
      "Epoch [1148/2000], Loss: 0.0000\n",
      "Epoch [1149/2000], Loss: 0.0000\n",
      "Epoch [1150/2000], Loss: 0.0001\n",
      "Epoch [1151/2000], Loss: 0.0000\n",
      "Epoch [1152/2000], Loss: 0.0000\n",
      "Epoch [1153/2000], Loss: 0.0000\n",
      "Epoch [1154/2000], Loss: 0.0000\n",
      "Epoch [1155/2000], Loss: 0.0001\n",
      "Epoch [1156/2000], Loss: 0.0000\n",
      "Epoch [1157/2000], Loss: 0.0000\n",
      "Epoch [1158/2000], Loss: 0.0001\n",
      "Epoch [1159/2000], Loss: 0.0000\n",
      "Epoch [1160/2000], Loss: 0.0001\n",
      "Epoch [1161/2000], Loss: 0.0000\n",
      "Epoch [1162/2000], Loss: 0.0001\n",
      "Epoch [1163/2000], Loss: 0.0001\n",
      "Epoch [1164/2000], Loss: 0.0000\n",
      "Epoch [1165/2000], Loss: 0.0000\n",
      "Epoch [1166/2000], Loss: 0.0000\n",
      "Epoch [1167/2000], Loss: 0.0000\n",
      "Epoch [1168/2000], Loss: 0.0000\n",
      "Epoch [1169/2000], Loss: 0.0000\n",
      "Epoch [1170/2000], Loss: 0.0000\n",
      "Epoch [1171/2000], Loss: 0.0000\n",
      "Epoch [1172/2000], Loss: 0.0000\n",
      "Epoch [1173/2000], Loss: 0.0000\n",
      "Epoch [1174/2000], Loss: 0.0000\n",
      "Epoch [1175/2000], Loss: 0.0000\n",
      "Epoch [1176/2000], Loss: 0.0000\n",
      "Epoch [1177/2000], Loss: 0.0000\n",
      "Epoch [1178/2000], Loss: 0.0001\n",
      "Epoch [1179/2000], Loss: 0.0000\n",
      "Epoch [1180/2000], Loss: 0.0000\n",
      "Epoch [1181/2000], Loss: 0.0000\n",
      "Epoch [1182/2000], Loss: 0.0000\n",
      "Epoch [1183/2000], Loss: 0.0000\n",
      "Epoch [1184/2000], Loss: 0.0001\n",
      "Epoch [1185/2000], Loss: 0.0001\n",
      "Epoch [1186/2000], Loss: 0.0000\n",
      "Epoch [1187/2000], Loss: 0.0001\n",
      "Epoch [1188/2000], Loss: 0.0000\n",
      "Epoch [1189/2000], Loss: 0.0000\n",
      "Epoch [1190/2000], Loss: 0.0000\n",
      "Epoch [1191/2000], Loss: 0.0001\n",
      "Epoch [1192/2000], Loss: 0.0000\n",
      "Epoch [1193/2000], Loss: 0.0000\n",
      "Epoch [1194/2000], Loss: 0.0000\n",
      "Epoch [1195/2000], Loss: 0.0000\n",
      "Epoch [1196/2000], Loss: 0.0000\n",
      "Epoch [1197/2000], Loss: 0.0000\n",
      "Epoch [1198/2000], Loss: 0.0000\n",
      "Epoch [1199/2000], Loss: 0.0000\n",
      "Epoch [1200/2000], Loss: 0.0000\n",
      "Epoch [1201/2000], Loss: 0.0000\n",
      "Epoch [1202/2000], Loss: 0.0000\n",
      "Epoch [1203/2000], Loss: 0.0000\n",
      "Epoch [1204/2000], Loss: 0.0001\n",
      "Epoch [1205/2000], Loss: 0.0000\n",
      "Epoch [1206/2000], Loss: 0.0000\n",
      "Epoch [1207/2000], Loss: 0.0000\n",
      "Epoch [1208/2000], Loss: 0.0000\n",
      "Epoch [1209/2000], Loss: 0.0000\n",
      "Epoch [1210/2000], Loss: 0.0000\n",
      "Epoch [1211/2000], Loss: 0.0000\n",
      "Epoch [1212/2000], Loss: 0.0000\n",
      "Epoch [1213/2000], Loss: 0.0000\n",
      "Epoch [1214/2000], Loss: 0.0000\n",
      "Epoch [1215/2000], Loss: 0.0000\n",
      "Epoch [1216/2000], Loss: 0.0000\n",
      "Epoch [1217/2000], Loss: 0.0000\n",
      "Epoch [1218/2000], Loss: 0.0000\n",
      "Epoch [1219/2000], Loss: 0.0000\n",
      "Epoch [1220/2000], Loss: 0.0000\n",
      "Epoch [1221/2000], Loss: 0.0000\n",
      "Epoch [1222/2000], Loss: 0.0000\n",
      "Epoch [1223/2000], Loss: 0.0000\n",
      "Epoch [1224/2000], Loss: 0.0000\n",
      "Epoch [1225/2000], Loss: 0.0000\n",
      "Epoch [1226/2000], Loss: 0.0000\n",
      "Epoch [1227/2000], Loss: 0.0000\n",
      "Epoch [1228/2000], Loss: 0.0000\n",
      "Epoch [1229/2000], Loss: 0.0000\n",
      "Epoch [1230/2000], Loss: 0.0000\n",
      "Epoch [1231/2000], Loss: 0.0000\n",
      "Epoch [1232/2000], Loss: 0.0000\n",
      "Epoch [1233/2000], Loss: 0.0000\n",
      "Epoch [1234/2000], Loss: 0.0000\n",
      "Epoch [1235/2000], Loss: 0.0000\n",
      "Epoch [1236/2000], Loss: 0.0000\n",
      "Epoch [1237/2000], Loss: 0.0000\n",
      "Epoch [1238/2000], Loss: 0.0000\n",
      "Epoch [1239/2000], Loss: 0.0000\n",
      "Epoch [1240/2000], Loss: 0.0000\n",
      "Epoch [1241/2000], Loss: 0.0000\n",
      "Epoch [1242/2000], Loss: 0.0000\n",
      "Epoch [1243/2000], Loss: 0.0000\n",
      "Epoch [1244/2000], Loss: 0.0000\n",
      "Epoch [1245/2000], Loss: 0.0000\n",
      "Epoch [1246/2000], Loss: 0.0000\n",
      "Epoch [1247/2000], Loss: 0.0000\n",
      "Epoch [1248/2000], Loss: 0.0000\n",
      "Epoch [1249/2000], Loss: 0.0000\n",
      "Epoch [1250/2000], Loss: 0.0000\n",
      "Epoch [1251/2000], Loss: 0.0000\n",
      "Epoch [1252/2000], Loss: 0.0000\n",
      "Epoch [1253/2000], Loss: 0.0000\n",
      "Epoch [1254/2000], Loss: 0.0000\n",
      "Epoch [1255/2000], Loss: 0.0000\n",
      "Epoch [1256/2000], Loss: 0.0000\n",
      "Epoch [1257/2000], Loss: 0.0000\n",
      "Epoch [1258/2000], Loss: 0.0000\n",
      "Epoch [1259/2000], Loss: 0.0000\n",
      "Epoch [1260/2000], Loss: 0.0000\n",
      "Epoch [1261/2000], Loss: 0.0000\n",
      "Epoch [1262/2000], Loss: 0.0000\n",
      "Epoch [1263/2000], Loss: 0.0000\n",
      "Epoch [1264/2000], Loss: 0.0000\n",
      "Epoch [1265/2000], Loss: 0.0000\n",
      "Epoch [1266/2000], Loss: 0.0000\n",
      "Epoch [1267/2000], Loss: 0.0000\n",
      "Epoch [1268/2000], Loss: 0.0000\n",
      "Epoch [1269/2000], Loss: 0.0000\n",
      "Epoch [1270/2000], Loss: 0.0000\n",
      "Epoch [1271/2000], Loss: 0.0000\n",
      "Epoch [1272/2000], Loss: 0.0000\n",
      "Epoch [1273/2000], Loss: 0.0000\n",
      "Epoch [1274/2000], Loss: 0.0000\n",
      "Epoch [1275/2000], Loss: 0.0000\n",
      "Epoch [1276/2000], Loss: 0.0000\n",
      "Epoch [1277/2000], Loss: 0.0000\n",
      "Epoch [1278/2000], Loss: 0.0000\n",
      "Epoch [1279/2000], Loss: 0.0000\n",
      "Epoch [1280/2000], Loss: 0.0000\n",
      "Epoch [1281/2000], Loss: 0.0000\n",
      "Epoch [1282/2000], Loss: 0.0000\n",
      "Epoch [1283/2000], Loss: 0.0000\n",
      "Epoch [1284/2000], Loss: 0.0000\n",
      "Epoch [1285/2000], Loss: 0.0000\n",
      "Epoch [1286/2000], Loss: 0.0000\n",
      "Epoch [1287/2000], Loss: 0.0000\n",
      "Epoch [1288/2000], Loss: 0.0000\n",
      "Epoch [1289/2000], Loss: 0.0000\n",
      "Epoch [1290/2000], Loss: 0.0000\n",
      "Epoch [1291/2000], Loss: 0.0000\n",
      "Epoch [1292/2000], Loss: 0.0000\n",
      "Epoch [1293/2000], Loss: 0.0000\n",
      "Epoch [1294/2000], Loss: 0.0000\n",
      "Epoch [1295/2000], Loss: 0.0000\n",
      "Epoch [1296/2000], Loss: 0.0000\n",
      "Epoch [1297/2000], Loss: 0.0000\n",
      "Epoch [1298/2000], Loss: 0.0000\n",
      "Epoch [1299/2000], Loss: 0.0000\n",
      "Epoch [1300/2000], Loss: 0.0000\n",
      "Epoch [1301/2000], Loss: 0.0000\n",
      "Epoch [1302/2000], Loss: 0.0000\n",
      "Epoch [1303/2000], Loss: 0.0000\n",
      "Epoch [1304/2000], Loss: 0.0000\n",
      "Epoch [1305/2000], Loss: 0.0000\n",
      "Epoch [1306/2000], Loss: 0.0000\n",
      "Epoch [1307/2000], Loss: 0.0000\n",
      "Epoch [1308/2000], Loss: 0.0000\n",
      "Epoch [1309/2000], Loss: 0.0000\n",
      "Epoch [1310/2000], Loss: 0.0000\n",
      "Epoch [1311/2000], Loss: 0.0000\n",
      "Epoch [1312/2000], Loss: 0.0000\n",
      "Epoch [1313/2000], Loss: 0.0000\n",
      "Epoch [1314/2000], Loss: 0.0000\n",
      "Epoch [1315/2000], Loss: 0.0000\n",
      "Epoch [1316/2000], Loss: 0.0000\n",
      "Epoch [1317/2000], Loss: 0.0000\n",
      "Epoch [1318/2000], Loss: 0.0000\n",
      "Epoch [1319/2000], Loss: 0.0000\n",
      "Epoch [1320/2000], Loss: 0.0000\n",
      "Epoch [1321/2000], Loss: 0.0000\n",
      "Epoch [1322/2000], Loss: 0.0000\n",
      "Epoch [1323/2000], Loss: 0.0000\n",
      "Epoch [1324/2000], Loss: 0.0000\n",
      "Epoch [1325/2000], Loss: 0.0000\n",
      "Epoch [1326/2000], Loss: 0.0000\n",
      "Epoch [1327/2000], Loss: 0.0000\n",
      "Epoch [1328/2000], Loss: 0.0000\n",
      "Epoch [1329/2000], Loss: 0.0000\n",
      "Epoch [1330/2000], Loss: 0.0000\n",
      "Epoch [1331/2000], Loss: 0.0000\n",
      "Epoch [1332/2000], Loss: 0.0000\n",
      "Epoch [1333/2000], Loss: 0.0000\n",
      "Epoch [1334/2000], Loss: 0.0000\n",
      "Epoch [1335/2000], Loss: 0.0000\n",
      "Epoch [1336/2000], Loss: 0.0000\n",
      "Epoch [1337/2000], Loss: 0.0000\n",
      "Epoch [1338/2000], Loss: 0.0000\n",
      "Epoch [1339/2000], Loss: 0.0000\n",
      "Epoch [1340/2000], Loss: 0.0000\n",
      "Epoch [1341/2000], Loss: 0.0000\n",
      "Epoch [1342/2000], Loss: 0.0000\n",
      "Epoch [1343/2000], Loss: 0.0000\n",
      "Epoch [1344/2000], Loss: 0.0000\n",
      "Epoch [1345/2000], Loss: 0.0000\n",
      "Epoch [1346/2000], Loss: 0.0000\n",
      "Epoch [1347/2000], Loss: 0.0000\n",
      "Epoch [1348/2000], Loss: 0.0000\n",
      "Epoch [1349/2000], Loss: 0.0000\n",
      "Epoch [1350/2000], Loss: 0.0000\n",
      "Epoch [1351/2000], Loss: 0.0000\n",
      "Epoch [1352/2000], Loss: 0.0000\n",
      "Epoch [1353/2000], Loss: 0.0000\n",
      "Epoch [1354/2000], Loss: 0.0000\n",
      "Epoch [1355/2000], Loss: 0.0000\n",
      "Epoch [1356/2000], Loss: 0.0000\n",
      "Epoch [1357/2000], Loss: 0.0000\n",
      "Epoch [1358/2000], Loss: 0.0000\n",
      "Epoch [1359/2000], Loss: 0.0000\n",
      "Epoch [1360/2000], Loss: 0.0000\n",
      "Epoch [1361/2000], Loss: 0.0000\n",
      "Epoch [1362/2000], Loss: 0.0000\n",
      "Epoch [1363/2000], Loss: 0.0000\n",
      "Epoch [1364/2000], Loss: 0.0000\n",
      "Epoch [1365/2000], Loss: 0.0000\n",
      "Epoch [1366/2000], Loss: 0.0000\n",
      "Epoch [1367/2000], Loss: 0.0000\n",
      "Epoch [1368/2000], Loss: 0.0000\n",
      "Epoch [1369/2000], Loss: 0.0000\n",
      "Epoch [1370/2000], Loss: 0.0000\n",
      "Epoch [1371/2000], Loss: 0.0000\n",
      "Epoch [1372/2000], Loss: 0.0000\n",
      "Epoch [1373/2000], Loss: 0.0000\n",
      "Epoch [1374/2000], Loss: 0.0000\n",
      "Epoch [1375/2000], Loss: 0.0000\n",
      "Epoch [1376/2000], Loss: 0.0000\n",
      "Epoch [1377/2000], Loss: 0.0000\n",
      "Epoch [1378/2000], Loss: 0.0000\n",
      "Epoch [1379/2000], Loss: 0.0000\n",
      "Epoch [1380/2000], Loss: 0.0000\n",
      "Epoch [1381/2000], Loss: 0.0000\n",
      "Epoch [1382/2000], Loss: 0.0000\n",
      "Epoch [1383/2000], Loss: 0.0000\n",
      "Epoch [1384/2000], Loss: 0.0000\n",
      "Epoch [1385/2000], Loss: 0.0000\n",
      "Epoch [1386/2000], Loss: 0.0000\n",
      "Epoch [1387/2000], Loss: 0.0000\n",
      "Epoch [1388/2000], Loss: 0.0000\n",
      "Epoch [1389/2000], Loss: 0.0000\n",
      "Epoch [1390/2000], Loss: 0.0000\n",
      "Epoch [1391/2000], Loss: 0.0000\n",
      "Epoch [1392/2000], Loss: 0.0000\n",
      "Epoch [1393/2000], Loss: 0.0000\n",
      "Epoch [1394/2000], Loss: 0.0000\n",
      "Epoch [1395/2000], Loss: 0.0000\n",
      "Epoch [1396/2000], Loss: 0.0000\n",
      "Epoch [1397/2000], Loss: 0.0000\n",
      "Epoch [1398/2000], Loss: 0.0000\n",
      "Epoch [1399/2000], Loss: 0.0000\n",
      "Epoch [1400/2000], Loss: 0.0000\n",
      "Epoch [1401/2000], Loss: 0.0000\n",
      "Epoch [1402/2000], Loss: 0.0000\n",
      "Epoch [1403/2000], Loss: 0.0000\n",
      "Epoch [1404/2000], Loss: 0.0000\n",
      "Epoch [1405/2000], Loss: 0.0000\n",
      "Epoch [1406/2000], Loss: 0.0000\n",
      "Epoch [1407/2000], Loss: 0.0000\n",
      "Epoch [1408/2000], Loss: 0.0000\n",
      "Epoch [1409/2000], Loss: 0.0000\n",
      "Epoch [1410/2000], Loss: 0.0000\n",
      "Epoch [1411/2000], Loss: 0.0000\n",
      "Epoch [1412/2000], Loss: 0.0000\n",
      "Epoch [1413/2000], Loss: 0.0000\n",
      "Epoch [1414/2000], Loss: 0.0000\n",
      "Epoch [1415/2000], Loss: 0.0000\n",
      "Epoch [1416/2000], Loss: 0.0000\n",
      "Epoch [1417/2000], Loss: 0.0000\n",
      "Epoch [1418/2000], Loss: 0.0000\n",
      "Epoch [1419/2000], Loss: 0.0000\n",
      "Epoch [1420/2000], Loss: 0.0000\n",
      "Epoch [1421/2000], Loss: 0.0000\n",
      "Epoch [1422/2000], Loss: 0.0000\n",
      "Epoch [1423/2000], Loss: 0.0000\n",
      "Epoch [1424/2000], Loss: 0.0000\n",
      "Epoch [1425/2000], Loss: 0.0000\n",
      "Epoch [1426/2000], Loss: 0.0000\n",
      "Epoch [1427/2000], Loss: 0.0000\n",
      "Epoch [1428/2000], Loss: 0.0000\n",
      "Epoch [1429/2000], Loss: 0.0000\n",
      "Epoch [1430/2000], Loss: 0.0000\n",
      "Epoch [1431/2000], Loss: 0.0000\n",
      "Epoch [1432/2000], Loss: 0.0000\n",
      "Epoch [1433/2000], Loss: 0.0000\n",
      "Epoch [1434/2000], Loss: 0.0000\n",
      "Epoch [1435/2000], Loss: 0.0000\n",
      "Epoch [1436/2000], Loss: 0.0000\n",
      "Epoch [1437/2000], Loss: 0.0000\n",
      "Epoch [1438/2000], Loss: 0.0000\n",
      "Epoch [1439/2000], Loss: 0.0000\n",
      "Epoch [1440/2000], Loss: 0.0000\n",
      "Epoch [1441/2000], Loss: 0.0000\n",
      "Epoch [1442/2000], Loss: 0.0000\n",
      "Epoch [1443/2000], Loss: 0.0000\n",
      "Epoch [1444/2000], Loss: 0.0000\n",
      "Epoch [1445/2000], Loss: 0.0000\n",
      "Epoch [1446/2000], Loss: 0.0000\n",
      "Epoch [1447/2000], Loss: 0.0000\n",
      "Epoch [1448/2000], Loss: 0.0000\n",
      "Epoch [1449/2000], Loss: 0.0000\n",
      "Epoch [1450/2000], Loss: 0.0000\n",
      "Epoch [1451/2000], Loss: 0.0000\n",
      "Epoch [1452/2000], Loss: 0.0000\n",
      "Epoch [1453/2000], Loss: 0.0000\n",
      "Epoch [1454/2000], Loss: 0.0000\n",
      "Epoch [1455/2000], Loss: 0.0000\n",
      "Epoch [1456/2000], Loss: 0.0000\n",
      "Epoch [1457/2000], Loss: 0.0000\n",
      "Epoch [1458/2000], Loss: 0.0000\n",
      "Epoch [1459/2000], Loss: 0.0000\n",
      "Epoch [1460/2000], Loss: 0.0000\n",
      "Epoch [1461/2000], Loss: 0.0000\n",
      "Epoch [1462/2000], Loss: 0.0000\n",
      "Epoch [1463/2000], Loss: 0.0000\n",
      "Epoch [1464/2000], Loss: 0.0000\n",
      "Epoch [1465/2000], Loss: 0.0000\n",
      "Epoch [1466/2000], Loss: 0.0000\n",
      "Epoch [1467/2000], Loss: 0.0000\n",
      "Epoch [1468/2000], Loss: 0.0000\n",
      "Epoch [1469/2000], Loss: 0.0000\n",
      "Epoch [1470/2000], Loss: 0.0000\n",
      "Epoch [1471/2000], Loss: 0.0000\n",
      "Epoch [1472/2000], Loss: 0.0000\n",
      "Epoch [1473/2000], Loss: 0.0000\n",
      "Epoch [1474/2000], Loss: 0.0000\n",
      "Epoch [1475/2000], Loss: 0.0000\n",
      "Epoch [1476/2000], Loss: 0.0000\n",
      "Epoch [1477/2000], Loss: 0.0000\n",
      "Epoch [1478/2000], Loss: 0.0000\n",
      "Epoch [1479/2000], Loss: 0.0000\n",
      "Epoch [1480/2000], Loss: 0.0000\n",
      "Epoch [1481/2000], Loss: 0.0000\n",
      "Epoch [1482/2000], Loss: 0.0000\n",
      "Epoch [1483/2000], Loss: 0.0000\n",
      "Epoch [1484/2000], Loss: 0.0000\n",
      "Epoch [1485/2000], Loss: 0.0000\n",
      "Epoch [1486/2000], Loss: 0.0000\n",
      "Epoch [1487/2000], Loss: 0.0000\n",
      "Epoch [1488/2000], Loss: 0.0000\n",
      "Epoch [1489/2000], Loss: 0.0000\n",
      "Epoch [1490/2000], Loss: 0.0000\n",
      "Epoch [1491/2000], Loss: 0.0000\n",
      "Epoch [1492/2000], Loss: 0.0000\n",
      "Epoch [1493/2000], Loss: 0.0000\n",
      "Epoch [1494/2000], Loss: 0.0000\n",
      "Epoch [1495/2000], Loss: 0.0000\n",
      "Epoch [1496/2000], Loss: 0.0000\n",
      "Epoch [1497/2000], Loss: 0.0000\n",
      "Epoch [1498/2000], Loss: 0.0000\n",
      "Epoch [1499/2000], Loss: 0.0000\n",
      "Epoch [1500/2000], Loss: 0.0000\n",
      "Epoch [1501/2000], Loss: 0.0000\n",
      "Epoch [1502/2000], Loss: 0.0000\n",
      "Epoch [1503/2000], Loss: 0.0000\n",
      "Epoch [1504/2000], Loss: 0.0000\n",
      "Epoch [1505/2000], Loss: 0.0000\n",
      "Epoch [1506/2000], Loss: 0.0000\n",
      "Epoch [1507/2000], Loss: 0.0000\n",
      "Epoch [1508/2000], Loss: 0.0000\n",
      "Epoch [1509/2000], Loss: 0.0000\n",
      "Epoch [1510/2000], Loss: 0.0000\n",
      "Epoch [1511/2000], Loss: 0.0000\n",
      "Epoch [1512/2000], Loss: 0.0000\n",
      "Epoch [1513/2000], Loss: 0.0000\n",
      "Epoch [1514/2000], Loss: 0.0000\n",
      "Epoch [1515/2000], Loss: 0.0000\n",
      "Epoch [1516/2000], Loss: 0.0000\n",
      "Epoch [1517/2000], Loss: 0.0000\n",
      "Epoch [1518/2000], Loss: 0.0000\n",
      "Epoch [1519/2000], Loss: 0.0000\n",
      "Epoch [1520/2000], Loss: 0.0000\n",
      "Epoch [1521/2000], Loss: 0.0000\n",
      "Epoch [1522/2000], Loss: 0.0000\n",
      "Epoch [1523/2000], Loss: 0.0000\n",
      "Epoch [1524/2000], Loss: 0.0000\n",
      "Epoch [1525/2000], Loss: 0.0000\n",
      "Epoch [1526/2000], Loss: 0.0000\n",
      "Epoch [1527/2000], Loss: 0.0000\n",
      "Epoch [1528/2000], Loss: 0.0000\n",
      "Epoch [1529/2000], Loss: 0.0000\n",
      "Epoch [1530/2000], Loss: 0.0000\n",
      "Epoch [1531/2000], Loss: 0.0000\n",
      "Epoch [1532/2000], Loss: 0.0000\n",
      "Epoch [1533/2000], Loss: 0.0000\n",
      "Epoch [1534/2000], Loss: 0.0000\n",
      "Epoch [1535/2000], Loss: 0.0000\n",
      "Epoch [1536/2000], Loss: 0.0000\n",
      "Epoch [1537/2000], Loss: 0.0000\n",
      "Epoch [1538/2000], Loss: 0.0000\n",
      "Epoch [1539/2000], Loss: 0.0000\n",
      "Epoch [1540/2000], Loss: 0.0000\n",
      "Epoch [1541/2000], Loss: 0.0000\n",
      "Epoch [1542/2000], Loss: 0.0000\n",
      "Epoch [1543/2000], Loss: 0.0000\n",
      "Epoch [1544/2000], Loss: 0.0000\n",
      "Epoch [1545/2000], Loss: 0.0000\n",
      "Epoch [1546/2000], Loss: 0.0000\n",
      "Epoch [1547/2000], Loss: 0.0000\n",
      "Epoch [1548/2000], Loss: 0.0000\n",
      "Epoch [1549/2000], Loss: 0.0000\n",
      "Epoch [1550/2000], Loss: 0.0000\n",
      "Epoch [1551/2000], Loss: 0.0000\n",
      "Epoch [1552/2000], Loss: 0.0000\n",
      "Epoch [1553/2000], Loss: 0.0000\n",
      "Epoch [1554/2000], Loss: 0.0000\n",
      "Epoch [1555/2000], Loss: 0.0000\n",
      "Epoch [1556/2000], Loss: 0.0000\n",
      "Epoch [1557/2000], Loss: 0.0000\n",
      "Epoch [1558/2000], Loss: 0.0000\n",
      "Epoch [1559/2000], Loss: 0.0000\n",
      "Epoch [1560/2000], Loss: 0.0000\n",
      "Epoch [1561/2000], Loss: 0.0000\n",
      "Epoch [1562/2000], Loss: 0.0000\n",
      "Epoch [1563/2000], Loss: 0.0000\n",
      "Epoch [1564/2000], Loss: 0.0000\n",
      "Epoch [1565/2000], Loss: 0.0000\n",
      "Epoch [1566/2000], Loss: 0.0000\n",
      "Epoch [1567/2000], Loss: 0.0000\n",
      "Epoch [1568/2000], Loss: 0.0000\n",
      "Epoch [1569/2000], Loss: 0.0000\n",
      "Epoch [1570/2000], Loss: 0.0000\n",
      "Epoch [1571/2000], Loss: 0.0000\n",
      "Epoch [1572/2000], Loss: 0.0000\n",
      "Epoch [1573/2000], Loss: 0.0000\n",
      "Epoch [1574/2000], Loss: 0.0000\n",
      "Epoch [1575/2000], Loss: 0.0000\n",
      "Epoch [1576/2000], Loss: 0.0000\n",
      "Epoch [1577/2000], Loss: 0.0000\n",
      "Epoch [1578/2000], Loss: 0.0000\n",
      "Epoch [1579/2000], Loss: 0.0000\n",
      "Epoch [1580/2000], Loss: 0.0000\n",
      "Epoch [1581/2000], Loss: 0.0000\n",
      "Epoch [1582/2000], Loss: 0.0000\n",
      "Epoch [1583/2000], Loss: 0.0000\n",
      "Epoch [1584/2000], Loss: 0.0000\n",
      "Epoch [1585/2000], Loss: 0.0000\n",
      "Epoch [1586/2000], Loss: 0.0000\n",
      "Epoch [1587/2000], Loss: 0.0000\n",
      "Epoch [1588/2000], Loss: 0.0000\n",
      "Epoch [1589/2000], Loss: 0.0000\n",
      "Epoch [1590/2000], Loss: 0.0000\n",
      "Epoch [1591/2000], Loss: 0.0000\n",
      "Epoch [1592/2000], Loss: 0.0000\n",
      "Epoch [1593/2000], Loss: 0.0000\n",
      "Epoch [1594/2000], Loss: 0.0000\n",
      "Epoch [1595/2000], Loss: 0.0000\n",
      "Epoch [1596/2000], Loss: 0.0000\n",
      "Epoch [1597/2000], Loss: 0.0000\n",
      "Epoch [1598/2000], Loss: 0.0000\n",
      "Epoch [1599/2000], Loss: 0.0000\n",
      "Epoch [1600/2000], Loss: 0.0000\n",
      "Epoch [1601/2000], Loss: 0.0000\n",
      "Epoch [1602/2000], Loss: 0.0000\n",
      "Epoch [1603/2000], Loss: 0.0000\n",
      "Epoch [1604/2000], Loss: 0.0000\n",
      "Epoch [1605/2000], Loss: 0.0000\n",
      "Epoch [1606/2000], Loss: 0.0000\n",
      "Epoch [1607/2000], Loss: 0.0000\n",
      "Epoch [1608/2000], Loss: 0.0000\n",
      "Epoch [1609/2000], Loss: 0.0000\n",
      "Epoch [1610/2000], Loss: 0.0000\n",
      "Epoch [1611/2000], Loss: 0.0000\n",
      "Epoch [1612/2000], Loss: 0.0000\n",
      "Epoch [1613/2000], Loss: 0.0000\n",
      "Epoch [1614/2000], Loss: 0.0000\n",
      "Epoch [1615/2000], Loss: 0.0000\n",
      "Epoch [1616/2000], Loss: 0.0000\n",
      "Epoch [1617/2000], Loss: 0.0000\n",
      "Epoch [1618/2000], Loss: 0.0000\n",
      "Epoch [1619/2000], Loss: 0.0000\n",
      "Epoch [1620/2000], Loss: 0.0000\n",
      "Epoch [1621/2000], Loss: 0.0000\n",
      "Epoch [1622/2000], Loss: 0.0000\n",
      "Epoch [1623/2000], Loss: 0.0000\n",
      "Epoch [1624/2000], Loss: 0.0000\n",
      "Epoch [1625/2000], Loss: 0.0000\n",
      "Epoch [1626/2000], Loss: 0.0000\n",
      "Epoch [1627/2000], Loss: 0.0000\n",
      "Epoch [1628/2000], Loss: 0.0000\n",
      "Epoch [1629/2000], Loss: 0.0000\n",
      "Epoch [1630/2000], Loss: 0.0000\n",
      "Epoch [1631/2000], Loss: 0.0000\n",
      "Epoch [1632/2000], Loss: 0.0000\n",
      "Epoch [1633/2000], Loss: 0.0000\n",
      "Epoch [1634/2000], Loss: 0.0000\n",
      "Epoch [1635/2000], Loss: 0.0000\n",
      "Epoch [1636/2000], Loss: 0.0000\n",
      "Epoch [1637/2000], Loss: 0.0000\n",
      "Epoch [1638/2000], Loss: 0.0000\n",
      "Epoch [1639/2000], Loss: 0.0000\n",
      "Epoch [1640/2000], Loss: 0.0000\n",
      "Epoch [1641/2000], Loss: 0.0000\n",
      "Epoch [1642/2000], Loss: 0.0000\n",
      "Epoch [1643/2000], Loss: 0.0000\n",
      "Epoch [1644/2000], Loss: 0.0000\n",
      "Epoch [1645/2000], Loss: 0.0000\n",
      "Epoch [1646/2000], Loss: 0.0000\n",
      "Epoch [1647/2000], Loss: 0.0000\n",
      "Epoch [1648/2000], Loss: 0.0000\n",
      "Epoch [1649/2000], Loss: 0.0000\n",
      "Epoch [1650/2000], Loss: 0.0000\n",
      "Epoch [1651/2000], Loss: 0.0000\n",
      "Epoch [1652/2000], Loss: 0.0000\n",
      "Epoch [1653/2000], Loss: 0.0000\n",
      "Epoch [1654/2000], Loss: 0.0000\n",
      "Epoch [1655/2000], Loss: 0.0000\n",
      "Epoch [1656/2000], Loss: 0.0000\n",
      "Epoch [1657/2000], Loss: 0.0000\n",
      "Epoch [1658/2000], Loss: 0.0000\n",
      "Epoch [1659/2000], Loss: 0.0000\n",
      "Epoch [1660/2000], Loss: 0.0000\n",
      "Epoch [1661/2000], Loss: 0.0000\n",
      "Epoch [1662/2000], Loss: 0.0000\n",
      "Epoch [1663/2000], Loss: 0.0000\n",
      "Epoch [1664/2000], Loss: 0.0000\n",
      "Epoch [1665/2000], Loss: 0.0000\n",
      "Epoch [1666/2000], Loss: 0.0000\n",
      "Epoch [1667/2000], Loss: 0.0000\n",
      "Epoch [1668/2000], Loss: 0.0000\n",
      "Epoch [1669/2000], Loss: 0.0000\n",
      "Epoch [1670/2000], Loss: 0.0000\n",
      "Epoch [1671/2000], Loss: 0.0000\n",
      "Epoch [1672/2000], Loss: 0.0000\n",
      "Epoch [1673/2000], Loss: 0.0000\n",
      "Epoch [1674/2000], Loss: 0.0000\n",
      "Epoch [1675/2000], Loss: 0.0000\n",
      "Epoch [1676/2000], Loss: 0.0000\n",
      "Epoch [1677/2000], Loss: 0.0000\n",
      "Epoch [1678/2000], Loss: 0.0000\n",
      "Epoch [1679/2000], Loss: 0.0000\n",
      "Epoch [1680/2000], Loss: 0.0000\n",
      "Epoch [1681/2000], Loss: 0.0000\n",
      "Epoch [1682/2000], Loss: 0.0000\n",
      "Epoch [1683/2000], Loss: 0.0000\n",
      "Epoch [1684/2000], Loss: 0.0000\n",
      "Epoch [1685/2000], Loss: 0.0000\n",
      "Epoch [1686/2000], Loss: 0.0000\n",
      "Epoch [1687/2000], Loss: 0.0000\n",
      "Epoch [1688/2000], Loss: 0.0000\n",
      "Epoch [1689/2000], Loss: 0.0000\n",
      "Epoch [1690/2000], Loss: 0.0000\n",
      "Epoch [1691/2000], Loss: 0.0000\n",
      "Epoch [1692/2000], Loss: 0.0000\n",
      "Epoch [1693/2000], Loss: 0.0000\n",
      "Epoch [1694/2000], Loss: 0.0000\n",
      "Epoch [1695/2000], Loss: 0.0000\n",
      "Epoch [1696/2000], Loss: 0.0000\n",
      "Epoch [1697/2000], Loss: 0.0000\n",
      "Epoch [1698/2000], Loss: 0.0000\n",
      "Epoch [1699/2000], Loss: 0.0000\n",
      "Epoch [1700/2000], Loss: 0.0000\n",
      "Epoch [1701/2000], Loss: 0.0000\n",
      "Epoch [1702/2000], Loss: 0.0000\n",
      "Epoch [1703/2000], Loss: 0.0000\n",
      "Epoch [1704/2000], Loss: 0.0000\n",
      "Epoch [1705/2000], Loss: 0.0000\n",
      "Epoch [1706/2000], Loss: 0.0000\n",
      "Epoch [1707/2000], Loss: 0.0000\n",
      "Epoch [1708/2000], Loss: 0.0000\n",
      "Epoch [1709/2000], Loss: 0.0000\n",
      "Epoch [1710/2000], Loss: 0.0000\n",
      "Epoch [1711/2000], Loss: 0.0000\n",
      "Epoch [1712/2000], Loss: 0.0000\n",
      "Epoch [1713/2000], Loss: 0.0000\n",
      "Epoch [1714/2000], Loss: 0.0000\n",
      "Epoch [1715/2000], Loss: 0.0000\n",
      "Epoch [1716/2000], Loss: 0.0000\n",
      "Epoch [1717/2000], Loss: 0.0000\n",
      "Epoch [1718/2000], Loss: 0.0000\n",
      "Epoch [1719/2000], Loss: 0.0000\n",
      "Epoch [1720/2000], Loss: 0.0000\n",
      "Epoch [1721/2000], Loss: 0.0000\n",
      "Epoch [1722/2000], Loss: 0.0000\n",
      "Epoch [1723/2000], Loss: 0.0000\n",
      "Epoch [1724/2000], Loss: 0.0000\n",
      "Epoch [1725/2000], Loss: 0.0000\n",
      "Epoch [1726/2000], Loss: 0.0000\n",
      "Epoch [1727/2000], Loss: 0.0000\n",
      "Epoch [1728/2000], Loss: 0.0000\n",
      "Epoch [1729/2000], Loss: 0.0000\n",
      "Epoch [1730/2000], Loss: 0.0000\n",
      "Epoch [1731/2000], Loss: 0.0000\n",
      "Epoch [1732/2000], Loss: 0.0000\n",
      "Epoch [1733/2000], Loss: 0.0000\n",
      "Epoch [1734/2000], Loss: 0.0000\n",
      "Epoch [1735/2000], Loss: 0.0000\n",
      "Epoch [1736/2000], Loss: 0.0000\n",
      "Epoch [1737/2000], Loss: 0.0000\n",
      "Epoch [1738/2000], Loss: 0.0000\n",
      "Epoch [1739/2000], Loss: 0.0000\n",
      "Epoch [1740/2000], Loss: 0.0000\n",
      "Epoch [1741/2000], Loss: 0.0000\n",
      "Epoch [1742/2000], Loss: 0.0000\n",
      "Epoch [1743/2000], Loss: 0.0000\n",
      "Epoch [1744/2000], Loss: 0.0000\n",
      "Epoch [1745/2000], Loss: 0.0000\n",
      "Epoch [1746/2000], Loss: 0.0000\n",
      "Epoch [1747/2000], Loss: 0.0000\n",
      "Epoch [1748/2000], Loss: 0.0000\n",
      "Epoch [1749/2000], Loss: 0.0000\n",
      "Epoch [1750/2000], Loss: 0.0000\n",
      "Epoch [1751/2000], Loss: 0.0000\n",
      "Epoch [1752/2000], Loss: 0.0000\n",
      "Epoch [1753/2000], Loss: 0.0000\n",
      "Epoch [1754/2000], Loss: 0.0000\n",
      "Epoch [1755/2000], Loss: 0.0000\n",
      "Epoch [1756/2000], Loss: 0.0000\n",
      "Epoch [1757/2000], Loss: 0.0000\n",
      "Epoch [1758/2000], Loss: 0.0000\n",
      "Epoch [1759/2000], Loss: 0.0000\n",
      "Epoch [1760/2000], Loss: 0.0000\n",
      "Epoch [1761/2000], Loss: 0.0000\n",
      "Epoch [1762/2000], Loss: 0.0000\n",
      "Epoch [1763/2000], Loss: 0.0000\n",
      "Epoch [1764/2000], Loss: 0.0000\n",
      "Epoch [1765/2000], Loss: 0.0000\n",
      "Epoch [1766/2000], Loss: 0.0000\n",
      "Epoch [1767/2000], Loss: 0.0000\n",
      "Epoch [1768/2000], Loss: 0.0000\n",
      "Epoch [1769/2000], Loss: 0.0000\n",
      "Epoch [1770/2000], Loss: 0.0000\n",
      "Epoch [1771/2000], Loss: 0.0000\n",
      "Epoch [1772/2000], Loss: 0.0000\n",
      "Epoch [1773/2000], Loss: 0.0000\n",
      "Epoch [1774/2000], Loss: 0.0000\n",
      "Epoch [1775/2000], Loss: 0.0000\n",
      "Epoch [1776/2000], Loss: 0.0000\n",
      "Epoch [1777/2000], Loss: 0.0000\n",
      "Epoch [1778/2000], Loss: 0.0000\n",
      "Epoch [1779/2000], Loss: 0.0000\n",
      "Epoch [1780/2000], Loss: 0.0000\n",
      "Epoch [1781/2000], Loss: 0.0000\n",
      "Epoch [1782/2000], Loss: 0.0000\n",
      "Epoch [1783/2000], Loss: 0.0000\n",
      "Epoch [1784/2000], Loss: 0.0000\n",
      "Epoch [1785/2000], Loss: 0.0000\n",
      "Epoch [1786/2000], Loss: 0.0000\n",
      "Epoch [1787/2000], Loss: 0.0000\n",
      "Epoch [1788/2000], Loss: 0.0000\n",
      "Epoch [1789/2000], Loss: 0.0000\n",
      "Epoch [1790/2000], Loss: 0.0000\n",
      "Epoch [1791/2000], Loss: 0.0000\n",
      "Epoch [1792/2000], Loss: 0.0000\n",
      "Epoch [1793/2000], Loss: 0.0000\n",
      "Epoch [1794/2000], Loss: 0.0000\n",
      "Epoch [1795/2000], Loss: 0.0000\n",
      "Epoch [1796/2000], Loss: 0.0000\n",
      "Epoch [1797/2000], Loss: 0.0000\n",
      "Epoch [1798/2000], Loss: 0.0000\n",
      "Epoch [1799/2000], Loss: 0.0000\n",
      "Epoch [1800/2000], Loss: 0.0000\n",
      "Epoch [1801/2000], Loss: 0.0000\n",
      "Epoch [1802/2000], Loss: 0.0000\n",
      "Epoch [1803/2000], Loss: 0.0000\n",
      "Epoch [1804/2000], Loss: 0.0000\n",
      "Epoch [1805/2000], Loss: 0.0000\n",
      "Epoch [1806/2000], Loss: 0.0000\n",
      "Epoch [1807/2000], Loss: 0.0000\n",
      "Epoch [1808/2000], Loss: 0.0000\n",
      "Epoch [1809/2000], Loss: 0.0000\n",
      "Epoch [1810/2000], Loss: 0.0000\n",
      "Epoch [1811/2000], Loss: 0.0000\n",
      "Epoch [1812/2000], Loss: 0.0000\n",
      "Epoch [1813/2000], Loss: 0.0000\n",
      "Epoch [1814/2000], Loss: 0.0000\n",
      "Epoch [1815/2000], Loss: 0.0000\n",
      "Epoch [1816/2000], Loss: 0.0000\n",
      "Epoch [1817/2000], Loss: 0.0000\n",
      "Epoch [1818/2000], Loss: 0.0000\n",
      "Epoch [1819/2000], Loss: 0.0000\n",
      "Epoch [1820/2000], Loss: 0.0000\n",
      "Epoch [1821/2000], Loss: 0.0000\n",
      "Epoch [1822/2000], Loss: 0.0000\n",
      "Epoch [1823/2000], Loss: 0.0000\n",
      "Epoch [1824/2000], Loss: 0.0000\n",
      "Epoch [1825/2000], Loss: 0.0000\n",
      "Epoch [1826/2000], Loss: 0.0000\n",
      "Epoch [1827/2000], Loss: 0.0000\n",
      "Epoch [1828/2000], Loss: 0.0000\n",
      "Epoch [1829/2000], Loss: 0.0000\n",
      "Epoch [1830/2000], Loss: 0.0000\n",
      "Epoch [1831/2000], Loss: 0.0000\n",
      "Epoch [1832/2000], Loss: 0.0000\n",
      "Epoch [1833/2000], Loss: 0.0000\n",
      "Epoch [1834/2000], Loss: 0.0000\n",
      "Epoch [1835/2000], Loss: 0.0000\n",
      "Epoch [1836/2000], Loss: 0.0000\n",
      "Epoch [1837/2000], Loss: 0.0000\n",
      "Epoch [1838/2000], Loss: 0.0000\n",
      "Epoch [1839/2000], Loss: 0.0000\n",
      "Epoch [1840/2000], Loss: 0.0000\n",
      "Epoch [1841/2000], Loss: 0.0000\n",
      "Epoch [1842/2000], Loss: 0.0000\n",
      "Epoch [1843/2000], Loss: 0.0000\n",
      "Epoch [1844/2000], Loss: 0.0000\n",
      "Epoch [1845/2000], Loss: 0.0000\n",
      "Epoch [1846/2000], Loss: 0.0000\n",
      "Epoch [1847/2000], Loss: 0.0000\n",
      "Epoch [1848/2000], Loss: 0.0000\n",
      "Epoch [1849/2000], Loss: 0.0000\n",
      "Epoch [1850/2000], Loss: 0.0000\n",
      "Epoch [1851/2000], Loss: 0.0000\n",
      "Epoch [1852/2000], Loss: 0.0000\n",
      "Epoch [1853/2000], Loss: 0.0000\n",
      "Epoch [1854/2000], Loss: 0.0000\n",
      "Epoch [1855/2000], Loss: 0.0000\n",
      "Epoch [1856/2000], Loss: 0.0000\n",
      "Epoch [1857/2000], Loss: 0.0000\n",
      "Epoch [1858/2000], Loss: 0.0000\n",
      "Epoch [1859/2000], Loss: 0.0000\n",
      "Epoch [1860/2000], Loss: 0.0000\n",
      "Epoch [1861/2000], Loss: 0.0000\n",
      "Epoch [1862/2000], Loss: 0.0000\n",
      "Epoch [1863/2000], Loss: 0.0000\n",
      "Epoch [1864/2000], Loss: 0.0000\n",
      "Epoch [1865/2000], Loss: 0.0000\n",
      "Epoch [1866/2000], Loss: 0.0000\n",
      "Epoch [1867/2000], Loss: 0.0000\n",
      "Epoch [1868/2000], Loss: 0.0000\n",
      "Epoch [1869/2000], Loss: 0.0000\n",
      "Epoch [1870/2000], Loss: 0.0000\n",
      "Epoch [1871/2000], Loss: 0.0000\n",
      "Epoch [1872/2000], Loss: 0.0000\n",
      "Epoch [1873/2000], Loss: 0.0000\n",
      "Epoch [1874/2000], Loss: 0.0000\n",
      "Epoch [1875/2000], Loss: 0.0000\n",
      "Epoch [1876/2000], Loss: 0.0000\n",
      "Epoch [1877/2000], Loss: 0.0000\n",
      "Epoch [1878/2000], Loss: 0.0000\n",
      "Epoch [1879/2000], Loss: 0.0000\n",
      "Epoch [1880/2000], Loss: 0.0000\n",
      "Epoch [1881/2000], Loss: 0.0000\n",
      "Epoch [1882/2000], Loss: 0.0000\n",
      "Epoch [1883/2000], Loss: 0.0000\n",
      "Epoch [1884/2000], Loss: 0.0000\n",
      "Epoch [1885/2000], Loss: 0.0000\n",
      "Epoch [1886/2000], Loss: 0.0000\n",
      "Epoch [1887/2000], Loss: 0.0000\n",
      "Epoch [1888/2000], Loss: 0.0000\n",
      "Epoch [1889/2000], Loss: 0.0000\n",
      "Epoch [1890/2000], Loss: 0.0000\n",
      "Epoch [1891/2000], Loss: 0.0000\n",
      "Epoch [1892/2000], Loss: 0.0000\n",
      "Epoch [1893/2000], Loss: 0.0000\n",
      "Epoch [1894/2000], Loss: 0.0000\n",
      "Epoch [1895/2000], Loss: 0.0000\n",
      "Epoch [1896/2000], Loss: 0.0000\n",
      "Epoch [1897/2000], Loss: 0.0000\n",
      "Epoch [1898/2000], Loss: 0.0000\n",
      "Epoch [1899/2000], Loss: 0.0000\n",
      "Epoch [1900/2000], Loss: 0.0000\n",
      "Epoch [1901/2000], Loss: 0.0000\n",
      "Epoch [1902/2000], Loss: 0.0000\n",
      "Epoch [1903/2000], Loss: 0.0000\n",
      "Epoch [1904/2000], Loss: 0.0000\n",
      "Epoch [1905/2000], Loss: 0.0000\n",
      "Epoch [1906/2000], Loss: 0.0000\n",
      "Epoch [1907/2000], Loss: 0.0000\n",
      "Epoch [1908/2000], Loss: 0.0000\n",
      "Epoch [1909/2000], Loss: 0.0000\n",
      "Epoch [1910/2000], Loss: 0.0000\n",
      "Epoch [1911/2000], Loss: 0.0000\n",
      "Epoch [1912/2000], Loss: 0.0000\n",
      "Epoch [1913/2000], Loss: 0.0000\n",
      "Epoch [1914/2000], Loss: 0.0000\n",
      "Epoch [1915/2000], Loss: 0.0000\n",
      "Epoch [1916/2000], Loss: 0.0000\n",
      "Epoch [1917/2000], Loss: 0.0000\n",
      "Epoch [1918/2000], Loss: 0.0000\n",
      "Epoch [1919/2000], Loss: 0.0000\n",
      "Epoch [1920/2000], Loss: 0.0000\n",
      "Epoch [1921/2000], Loss: 0.0000\n",
      "Epoch [1922/2000], Loss: 0.0000\n",
      "Epoch [1923/2000], Loss: 0.0000\n",
      "Epoch [1924/2000], Loss: 0.0000\n",
      "Epoch [1925/2000], Loss: 0.0000\n",
      "Epoch [1926/2000], Loss: 0.0000\n",
      "Epoch [1927/2000], Loss: 0.0000\n",
      "Epoch [1928/2000], Loss: 0.0000\n",
      "Epoch [1929/2000], Loss: 0.0000\n",
      "Epoch [1930/2000], Loss: 0.0000\n",
      "Epoch [1931/2000], Loss: 0.0000\n",
      "Epoch [1932/2000], Loss: 0.0000\n",
      "Epoch [1933/2000], Loss: 0.0000\n",
      "Epoch [1934/2000], Loss: 0.0000\n",
      "Epoch [1935/2000], Loss: 0.0000\n",
      "Epoch [1936/2000], Loss: 0.0000\n",
      "Epoch [1937/2000], Loss: 0.0000\n",
      "Epoch [1938/2000], Loss: 0.0000\n",
      "Epoch [1939/2000], Loss: 0.0000\n",
      "Epoch [1940/2000], Loss: 0.0000\n",
      "Epoch [1941/2000], Loss: 0.0000\n",
      "Epoch [1942/2000], Loss: 0.0000\n",
      "Epoch [1943/2000], Loss: 0.0000\n",
      "Epoch [1944/2000], Loss: 0.0000\n",
      "Epoch [1945/2000], Loss: 0.0000\n",
      "Epoch [1946/2000], Loss: 0.0000\n",
      "Epoch [1947/2000], Loss: 0.0000\n",
      "Epoch [1948/2000], Loss: 0.0000\n",
      "Epoch [1949/2000], Loss: 0.0000\n",
      "Epoch [1950/2000], Loss: 0.0000\n",
      "Epoch [1951/2000], Loss: 0.0000\n",
      "Epoch [1952/2000], Loss: 0.0000\n",
      "Epoch [1953/2000], Loss: 0.0000\n",
      "Epoch [1954/2000], Loss: 0.0000\n",
      "Epoch [1955/2000], Loss: 0.0000\n",
      "Epoch [1956/2000], Loss: 0.0000\n",
      "Epoch [1957/2000], Loss: 0.0000\n",
      "Epoch [1958/2000], Loss: 0.0000\n",
      "Epoch [1959/2000], Loss: 0.0000\n",
      "Epoch [1960/2000], Loss: 0.0000\n",
      "Epoch [1961/2000], Loss: 0.0000\n",
      "Epoch [1962/2000], Loss: 0.0000\n",
      "Epoch [1963/2000], Loss: 0.0000\n",
      "Epoch [1964/2000], Loss: 0.0000\n",
      "Epoch [1965/2000], Loss: 0.0000\n",
      "Epoch [1966/2000], Loss: 0.0000\n",
      "Epoch [1967/2000], Loss: 0.0000\n",
      "Epoch [1968/2000], Loss: 0.0000\n",
      "Epoch [1969/2000], Loss: 0.0000\n",
      "Epoch [1970/2000], Loss: 0.0000\n",
      "Epoch [1971/2000], Loss: 0.0000\n",
      "Epoch [1972/2000], Loss: 0.0000\n",
      "Epoch [1973/2000], Loss: 0.0000\n",
      "Epoch [1974/2000], Loss: 0.0000\n",
      "Epoch [1975/2000], Loss: 0.0000\n",
      "Epoch [1976/2000], Loss: 0.0000\n",
      "Epoch [1977/2000], Loss: 0.0000\n",
      "Epoch [1978/2000], Loss: 0.0000\n",
      "Epoch [1979/2000], Loss: 0.0000\n",
      "Epoch [1980/2000], Loss: 0.0000\n",
      "Epoch [1981/2000], Loss: 0.0000\n",
      "Epoch [1982/2000], Loss: 0.0000\n",
      "Epoch [1983/2000], Loss: 0.0000\n",
      "Epoch [1984/2000], Loss: 0.0000\n",
      "Epoch [1985/2000], Loss: 0.0000\n",
      "Epoch [1986/2000], Loss: 0.0000\n",
      "Epoch [1987/2000], Loss: 0.0000\n",
      "Epoch [1988/2000], Loss: 0.0000\n",
      "Epoch [1989/2000], Loss: 0.0000\n",
      "Epoch [1990/2000], Loss: 0.0000\n",
      "Epoch [1991/2000], Loss: 0.0000\n",
      "Epoch [1992/2000], Loss: 0.0000\n",
      "Epoch [1993/2000], Loss: 0.0000\n",
      "Epoch [1994/2000], Loss: 0.0000\n",
      "Epoch [1995/2000], Loss: 0.0000\n",
      "Epoch [1996/2000], Loss: 0.0000\n",
      "Epoch [1997/2000], Loss: 0.0000\n",
      "Epoch [1998/2000], Loss: 0.0000\n",
      "Epoch [1999/2000], Loss: 0.0000\n",
      "Epoch [2000/2000], Loss: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56        40\n",
      "           1       0.17      0.13      0.15        31\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.33        81\n",
      "   macro avg       0.24      0.23      0.24        81\n",
      "weighted avg       0.34      0.33      0.33        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 定义简单的全连接网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "# 加载数据\n",
    "train_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19-2\\processed\\train.npy')\n",
    "train_labels = np.argmax(np.load(r'D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/train_(327, 3).npy'), axis=1)\n",
    "test_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19-2\\processed\\test.npy')\n",
    "test_labels = np.argmax(np.load(r'D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/test_(81, 3).npy'), axis=1)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = SimpleNN(input_size=train_features.shape[1], hidden_size=100, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_preds = []\n",
    "    for features, labels in test_loader:\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.numpy())\n",
    "\n",
    "print(classification_report(test_labels.numpy(), np.array(all_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [10/21], Loss: 0.9832\n",
      "Epoch [1/20], Step [20/21], Loss: 1.0978\n",
      "Epoch [2/20], Step [10/21], Loss: 0.8860\n",
      "Epoch [2/20], Step [20/21], Loss: 0.9320\n",
      "Epoch [3/20], Step [10/21], Loss: 1.0217\n",
      "Epoch [3/20], Step [20/21], Loss: 0.8363\n",
      "Epoch [4/20], Step [10/21], Loss: 0.6254\n",
      "Epoch [4/20], Step [20/21], Loss: 0.6162\n",
      "Epoch [5/20], Step [10/21], Loss: 0.2534\n",
      "Epoch [5/20], Step [20/21], Loss: 0.7031\n",
      "Epoch [6/20], Step [10/21], Loss: 0.5240\n",
      "Epoch [6/20], Step [20/21], Loss: 0.6642\n",
      "Epoch [7/20], Step [10/21], Loss: 0.4316\n",
      "Epoch [7/20], Step [20/21], Loss: 0.2109\n",
      "Epoch [8/20], Step [10/21], Loss: 0.5112\n",
      "Epoch [8/20], Step [20/21], Loss: 0.0703\n",
      "Epoch [9/20], Step [10/21], Loss: 0.1233\n",
      "Epoch [9/20], Step [20/21], Loss: 0.1969\n",
      "Epoch [10/20], Step [10/21], Loss: 0.4150\n",
      "Epoch [10/20], Step [20/21], Loss: 0.0523\n",
      "Epoch [11/20], Step [10/21], Loss: 0.0536\n",
      "Epoch [11/20], Step [20/21], Loss: 0.0170\n",
      "Epoch [12/20], Step [10/21], Loss: 0.0094\n",
      "Epoch [12/20], Step [20/21], Loss: 0.0037\n",
      "Epoch [13/20], Step [10/21], Loss: 0.0051\n",
      "Epoch [13/20], Step [20/21], Loss: 0.0084\n",
      "Epoch [14/20], Step [10/21], Loss: 0.0076\n",
      "Epoch [14/20], Step [20/21], Loss: 0.0145\n",
      "Epoch [15/20], Step [10/21], Loss: 0.0838\n",
      "Epoch [15/20], Step [20/21], Loss: 0.0464\n",
      "Epoch [16/20], Step [10/21], Loss: 0.3396\n",
      "Epoch [16/20], Step [20/21], Loss: 0.1717\n",
      "Epoch [17/20], Step [10/21], Loss: 0.0180\n",
      "Epoch [17/20], Step [20/21], Loss: 0.0691\n",
      "Epoch [18/20], Step [10/21], Loss: 0.0060\n",
      "Epoch [18/20], Step [20/21], Loss: 0.0307\n",
      "Epoch [19/20], Step [10/21], Loss: 0.0166\n",
      "Epoch [19/20], Step [20/21], Loss: 0.0011\n",
      "Epoch [20/20], Step [10/21], Loss: 0.0016\n",
      "Epoch [20/20], Step [20/21], Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 参数设置\n",
    "input_size = 768  # 输入特征的维度\n",
    "hidden_size = 256  # LSTM的隐藏层大小\n",
    "num_layers = 1  # LSTM的层数\n",
    "num_classes = 3  # 输出类别数\n",
    "batch_size = 16  # 批大小\n",
    "learning_rate = 0.001\n",
    "num_epochs =20 # 训练周期数\n",
    "\n",
    "# 确保CUDA可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型定义\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = torch.matmul(hidden_states, self.weight)  # [batch_size, seq_len]\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        context = torch.sum(hidden_states * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        context, attn_weights = self.attention(out)\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "\n",
    "# 初始化模型并移至GPU\n",
    "model = BiLSTMWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# 加载数据\n",
    "train_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\processed\\updated_train.npy')\n",
    "train_labels = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\labels\\train_(327, 3).npy')\n",
    "train_dataset = TensorDataset(torch.Tensor(train_features), torch.Tensor(train_labels))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2716\n",
      "Macro F1 Score: 0.2397\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.42      0.35      0.38        40\n",
      "        Real       0.24      0.19      0.21        31\n",
      "  Unverified       0.09      0.20      0.12        10\n",
      "\n",
      "    accuracy                           0.27        81\n",
      "   macro avg       0.25      0.25      0.24        81\n",
      "weighted avg       0.31      0.27      0.29        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "test_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\processed\\updated_test.npy')\n",
    "test_labels = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\labels\\test_(81, 3).npy')\n",
    "test_dataset = TensorDataset(torch.Tensor(test_features), torch.Tensor(test_labels))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# 计算准确率\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# 计算总体的F1分数\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Macro F1 Score: {f1:.4f}')\n",
    "# 输出分类报告，确保包含了所有类别\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real', 'Unverified']))\n",
    "# print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [10/21], Loss: 1.0419\n",
      "Epoch [1/200], Step [20/21], Loss: 0.9589\n",
      "Epoch [2/200], Step [10/21], Loss: 1.0279\n",
      "Epoch [2/200], Step [20/21], Loss: 0.9194\n",
      "Epoch [3/200], Step [10/21], Loss: 0.8924\n",
      "Epoch [3/200], Step [20/21], Loss: 1.0839\n",
      "Epoch [4/200], Step [10/21], Loss: 1.0197\n",
      "Epoch [4/200], Step [20/21], Loss: 0.9460\n",
      "Epoch [5/200], Step [10/21], Loss: 0.9287\n",
      "Epoch [5/200], Step [20/21], Loss: 0.8796\n",
      "Epoch [6/200], Step [10/21], Loss: 0.7362\n",
      "Epoch [6/200], Step [20/21], Loss: 0.8040\n",
      "Epoch [7/200], Step [10/21], Loss: 0.8103\n",
      "Epoch [7/200], Step [20/21], Loss: 0.9105\n",
      "Epoch [8/200], Step [10/21], Loss: 0.7539\n",
      "Epoch [8/200], Step [20/21], Loss: 0.7111\n",
      "Epoch [9/200], Step [10/21], Loss: 0.7997\n",
      "Epoch [9/200], Step [20/21], Loss: 0.6732\n",
      "Epoch [10/200], Step [10/21], Loss: 0.6539\n",
      "Epoch [10/200], Step [20/21], Loss: 0.6724\n",
      "Epoch [11/200], Step [10/21], Loss: 0.7396\n",
      "Epoch [11/200], Step [20/21], Loss: 0.7676\n",
      "Epoch [12/200], Step [10/21], Loss: 0.5813\n",
      "Epoch [12/200], Step [20/21], Loss: 0.7653\n",
      "Epoch [13/200], Step [10/21], Loss: 1.0415\n",
      "Epoch [13/200], Step [20/21], Loss: 0.5778\n",
      "Epoch [14/200], Step [10/21], Loss: 0.5934\n",
      "Epoch [14/200], Step [20/21], Loss: 0.6732\n",
      "Epoch [15/200], Step [10/21], Loss: 0.5853\n",
      "Epoch [15/200], Step [20/21], Loss: 0.4687\n",
      "Epoch [16/200], Step [10/21], Loss: 0.5786\n",
      "Epoch [16/200], Step [20/21], Loss: 0.8475\n",
      "Epoch [17/200], Step [10/21], Loss: 0.7145\n",
      "Epoch [17/200], Step [20/21], Loss: 0.6248\n",
      "Epoch [18/200], Step [10/21], Loss: 0.4059\n",
      "Epoch [18/200], Step [20/21], Loss: 0.6845\n",
      "Epoch [19/200], Step [10/21], Loss: 0.3969\n",
      "Epoch [19/200], Step [20/21], Loss: 0.3342\n",
      "Epoch [20/200], Step [10/21], Loss: 0.2552\n",
      "Epoch [20/200], Step [20/21], Loss: 0.4385\n",
      "Epoch [21/200], Step [10/21], Loss: 0.1727\n",
      "Epoch [21/200], Step [20/21], Loss: 0.7715\n",
      "Epoch [22/200], Step [10/21], Loss: 0.3238\n",
      "Epoch [22/200], Step [20/21], Loss: 0.3785\n",
      "Epoch [23/200], Step [10/21], Loss: 0.1922\n",
      "Epoch [23/200], Step [20/21], Loss: 0.4141\n",
      "Epoch [24/200], Step [10/21], Loss: 0.6247\n",
      "Epoch [24/200], Step [20/21], Loss: 0.3685\n",
      "Epoch [25/200], Step [10/21], Loss: 0.3642\n",
      "Epoch [25/200], Step [20/21], Loss: 0.7940\n",
      "Epoch [26/200], Step [10/21], Loss: 0.4829\n",
      "Epoch [26/200], Step [20/21], Loss: 0.3095\n",
      "Epoch [27/200], Step [10/21], Loss: 0.2219\n",
      "Epoch [27/200], Step [20/21], Loss: 0.1601\n",
      "Epoch [28/200], Step [10/21], Loss: 0.0952\n",
      "Epoch [28/200], Step [20/21], Loss: 0.3430\n",
      "Epoch [29/200], Step [10/21], Loss: 0.1667\n",
      "Epoch [29/200], Step [20/21], Loss: 0.1517\n",
      "Epoch [30/200], Step [10/21], Loss: 0.1220\n",
      "Epoch [30/200], Step [20/21], Loss: 0.1398\n",
      "Epoch [31/200], Step [10/21], Loss: 0.0952\n",
      "Epoch [31/200], Step [20/21], Loss: 0.1741\n",
      "Epoch [32/200], Step [10/21], Loss: 0.1529\n",
      "Epoch [32/200], Step [20/21], Loss: 0.2367\n",
      "Epoch [33/200], Step [10/21], Loss: 0.1091\n",
      "Epoch [33/200], Step [20/21], Loss: 0.1369\n",
      "Epoch [34/200], Step [10/21], Loss: 0.0758\n",
      "Epoch [34/200], Step [20/21], Loss: 0.1348\n",
      "Epoch [35/200], Step [10/21], Loss: 0.0479\n",
      "Epoch [35/200], Step [20/21], Loss: 0.0895\n",
      "Epoch [36/200], Step [10/21], Loss: 0.0324\n",
      "Epoch [36/200], Step [20/21], Loss: 0.0285\n",
      "Epoch [37/200], Step [10/21], Loss: 0.0427\n",
      "Epoch [37/200], Step [20/21], Loss: 0.0626\n",
      "Epoch [38/200], Step [10/21], Loss: 0.0526\n",
      "Epoch [38/200], Step [20/21], Loss: 0.0437\n",
      "Epoch [39/200], Step [10/21], Loss: 0.1150\n",
      "Epoch [39/200], Step [20/21], Loss: 0.0380\n",
      "Epoch [40/200], Step [10/21], Loss: 0.0458\n",
      "Epoch [40/200], Step [20/21], Loss: 0.4135\n",
      "Epoch [41/200], Step [10/21], Loss: 0.0194\n",
      "Epoch [41/200], Step [20/21], Loss: 0.0522\n",
      "Epoch [42/200], Step [10/21], Loss: 0.0223\n",
      "Epoch [42/200], Step [20/21], Loss: 0.0305\n",
      "Epoch [43/200], Step [10/21], Loss: 0.0128\n",
      "Epoch [43/200], Step [20/21], Loss: 0.0185\n",
      "Epoch [44/200], Step [10/21], Loss: 0.0182\n",
      "Epoch [44/200], Step [20/21], Loss: 0.0201\n",
      "Epoch [45/200], Step [10/21], Loss: 0.0360\n",
      "Epoch [45/200], Step [20/21], Loss: 0.0224\n",
      "Epoch [46/200], Step [10/21], Loss: 0.0165\n",
      "Epoch [46/200], Step [20/21], Loss: 0.0083\n",
      "Epoch [47/200], Step [10/21], Loss: 0.0054\n",
      "Epoch [47/200], Step [20/21], Loss: 0.0160\n",
      "Epoch [48/200], Step [10/21], Loss: 0.0069\n",
      "Epoch [48/200], Step [20/21], Loss: 0.0071\n",
      "Epoch [49/200], Step [10/21], Loss: 0.0124\n",
      "Epoch [49/200], Step [20/21], Loss: 0.0029\n",
      "Epoch [50/200], Step [10/21], Loss: 0.0076\n",
      "Epoch [50/200], Step [20/21], Loss: 0.0048\n",
      "Epoch [51/200], Step [10/21], Loss: 0.0132\n",
      "Epoch [51/200], Step [20/21], Loss: 0.0072\n",
      "Epoch [52/200], Step [10/21], Loss: 0.0056\n",
      "Epoch [52/200], Step [20/21], Loss: 0.0044\n",
      "Epoch [53/200], Step [10/21], Loss: 0.0060\n",
      "Epoch [53/200], Step [20/21], Loss: 0.0020\n",
      "Epoch [54/200], Step [10/21], Loss: 0.0105\n",
      "Epoch [54/200], Step [20/21], Loss: 0.0216\n",
      "Epoch [55/200], Step [10/21], Loss: 0.0084\n",
      "Epoch [55/200], Step [20/21], Loss: 0.0167\n",
      "Epoch [56/200], Step [10/21], Loss: 0.0165\n",
      "Epoch [56/200], Step [20/21], Loss: 0.0587\n",
      "Epoch [57/200], Step [10/21], Loss: 0.0409\n",
      "Epoch [57/200], Step [20/21], Loss: 0.0119\n",
      "Epoch [58/200], Step [10/21], Loss: 0.0061\n",
      "Epoch [58/200], Step [20/21], Loss: 0.0083\n",
      "Epoch [59/200], Step [10/21], Loss: 0.0159\n",
      "Epoch [59/200], Step [20/21], Loss: 0.0035\n",
      "Epoch [60/200], Step [10/21], Loss: 0.0065\n",
      "Epoch [60/200], Step [20/21], Loss: 0.0062\n",
      "Epoch [61/200], Step [10/21], Loss: 0.0054\n",
      "Epoch [61/200], Step [20/21], Loss: 0.0027\n",
      "Epoch [62/200], Step [10/21], Loss: 0.0062\n",
      "Epoch [62/200], Step [20/21], Loss: 0.0019\n",
      "Epoch [63/200], Step [10/21], Loss: 0.0051\n",
      "Epoch [63/200], Step [20/21], Loss: 0.0024\n",
      "Epoch [64/200], Step [10/21], Loss: 0.0022\n",
      "Epoch [64/200], Step [20/21], Loss: 0.0028\n",
      "Epoch [65/200], Step [10/21], Loss: 0.0036\n",
      "Epoch [65/200], Step [20/21], Loss: 0.0032\n",
      "Epoch [66/200], Step [10/21], Loss: 0.0024\n",
      "Epoch [66/200], Step [20/21], Loss: 0.0025\n",
      "Epoch [67/200], Step [10/21], Loss: 0.0021\n",
      "Epoch [67/200], Step [20/21], Loss: 0.0022\n",
      "Epoch [68/200], Step [10/21], Loss: 0.0044\n",
      "Epoch [68/200], Step [20/21], Loss: 0.0049\n",
      "Epoch [69/200], Step [10/21], Loss: 0.0028\n",
      "Epoch [69/200], Step [20/21], Loss: 0.0014\n",
      "Epoch [70/200], Step [10/21], Loss: 0.0022\n",
      "Epoch [70/200], Step [20/21], Loss: 0.0011\n",
      "Epoch [71/200], Step [10/21], Loss: 0.0027\n",
      "Epoch [71/200], Step [20/21], Loss: 0.0018\n",
      "Epoch [72/200], Step [10/21], Loss: 0.0023\n",
      "Epoch [72/200], Step [20/21], Loss: 0.0044\n",
      "Epoch [73/200], Step [10/21], Loss: 0.0022\n",
      "Epoch [73/200], Step [20/21], Loss: 0.0011\n",
      "Epoch [74/200], Step [10/21], Loss: 0.0008\n",
      "Epoch [74/200], Step [20/21], Loss: 0.0014\n",
      "Epoch [75/200], Step [10/21], Loss: 0.0014\n",
      "Epoch [75/200], Step [20/21], Loss: 0.0014\n",
      "Epoch [76/200], Step [10/21], Loss: 0.0026\n",
      "Epoch [76/200], Step [20/21], Loss: 0.0016\n",
      "Epoch [77/200], Step [10/21], Loss: 0.0031\n",
      "Epoch [77/200], Step [20/21], Loss: 0.0006\n",
      "Epoch [78/200], Step [10/21], Loss: 0.0022\n",
      "Epoch [78/200], Step [20/21], Loss: 0.0029\n",
      "Epoch [79/200], Step [10/21], Loss: 0.0010\n",
      "Epoch [79/200], Step [20/21], Loss: 0.0009\n",
      "Epoch [80/200], Step [10/21], Loss: 0.0018\n",
      "Epoch [80/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [81/200], Step [10/21], Loss: 0.0018\n",
      "Epoch [81/200], Step [20/21], Loss: 0.0025\n",
      "Epoch [82/200], Step [10/21], Loss: 0.0015\n",
      "Epoch [82/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [83/200], Step [10/21], Loss: 0.0009\n",
      "Epoch [83/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [84/200], Step [10/21], Loss: 0.0010\n",
      "Epoch [84/200], Step [20/21], Loss: 0.0017\n",
      "Epoch [85/200], Step [10/21], Loss: 0.0021\n",
      "Epoch [85/200], Step [20/21], Loss: 0.0015\n",
      "Epoch [86/200], Step [10/21], Loss: 0.0009\n",
      "Epoch [86/200], Step [20/21], Loss: 0.0009\n",
      "Epoch [87/200], Step [10/21], Loss: 0.0007\n",
      "Epoch [87/200], Step [20/21], Loss: 0.0022\n",
      "Epoch [88/200], Step [10/21], Loss: 0.0011\n",
      "Epoch [88/200], Step [20/21], Loss: 0.0019\n",
      "Epoch [89/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [89/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [90/200], Step [10/21], Loss: 0.0022\n",
      "Epoch [90/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [91/200], Step [10/21], Loss: 0.0021\n",
      "Epoch [91/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [92/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [92/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [93/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [93/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [94/200], Step [10/21], Loss: 0.0023\n",
      "Epoch [94/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [95/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [95/200], Step [20/21], Loss: 0.0008\n",
      "Epoch [96/200], Step [10/21], Loss: 0.0007\n",
      "Epoch [96/200], Step [20/21], Loss: 0.0022\n",
      "Epoch [97/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [97/200], Step [20/21], Loss: 0.3171\n",
      "Epoch [98/200], Step [10/21], Loss: 0.0165\n",
      "Epoch [98/200], Step [20/21], Loss: 0.2342\n",
      "Epoch [99/200], Step [10/21], Loss: 0.7472\n",
      "Epoch [99/200], Step [20/21], Loss: 0.0867\n",
      "Epoch [100/200], Step [10/21], Loss: 0.1662\n",
      "Epoch [100/200], Step [20/21], Loss: 0.0406\n",
      "Epoch [101/200], Step [10/21], Loss: 0.0649\n",
      "Epoch [101/200], Step [20/21], Loss: 0.0381\n",
      "Epoch [102/200], Step [10/21], Loss: 0.0143\n",
      "Epoch [102/200], Step [20/21], Loss: 0.0084\n",
      "Epoch [103/200], Step [10/21], Loss: 0.0103\n",
      "Epoch [103/200], Step [20/21], Loss: 0.0062\n",
      "Epoch [104/200], Step [10/21], Loss: 0.0033\n",
      "Epoch [104/200], Step [20/21], Loss: 0.0048\n",
      "Epoch [105/200], Step [10/21], Loss: 0.0030\n",
      "Epoch [105/200], Step [20/21], Loss: 0.0024\n",
      "Epoch [106/200], Step [10/21], Loss: 0.0017\n",
      "Epoch [106/200], Step [20/21], Loss: 0.0023\n",
      "Epoch [107/200], Step [10/21], Loss: 0.0020\n",
      "Epoch [107/200], Step [20/21], Loss: 0.0024\n",
      "Epoch [108/200], Step [10/21], Loss: 0.0015\n",
      "Epoch [108/200], Step [20/21], Loss: 0.0022\n",
      "Epoch [109/200], Step [10/21], Loss: 0.0015\n",
      "Epoch [109/200], Step [20/21], Loss: 0.0024\n",
      "Epoch [110/200], Step [10/21], Loss: 0.0017\n",
      "Epoch [110/200], Step [20/21], Loss: 0.0045\n",
      "Epoch [111/200], Step [10/21], Loss: 0.0011\n",
      "Epoch [111/200], Step [20/21], Loss: 0.0015\n",
      "Epoch [112/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [112/200], Step [20/21], Loss: 0.0029\n",
      "Epoch [113/200], Step [10/21], Loss: 0.0024\n",
      "Epoch [113/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [114/200], Step [10/21], Loss: 0.0015\n",
      "Epoch [114/200], Step [20/21], Loss: 0.0010\n",
      "Epoch [115/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [115/200], Step [20/21], Loss: 0.0033\n",
      "Epoch [116/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [116/200], Step [20/21], Loss: 0.0013\n",
      "Epoch [117/200], Step [10/21], Loss: 0.0011\n",
      "Epoch [117/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [118/200], Step [10/21], Loss: 0.0025\n",
      "Epoch [118/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [119/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [119/200], Step [20/21], Loss: 0.0011\n",
      "Epoch [120/200], Step [10/21], Loss: 0.0016\n",
      "Epoch [120/200], Step [20/21], Loss: 0.0008\n",
      "Epoch [121/200], Step [10/21], Loss: 0.0011\n",
      "Epoch [121/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [122/200], Step [10/21], Loss: 0.0015\n",
      "Epoch [122/200], Step [20/21], Loss: 0.0015\n",
      "Epoch [123/200], Step [10/21], Loss: 0.0017\n",
      "Epoch [123/200], Step [20/21], Loss: 0.0008\n",
      "Epoch [124/200], Step [10/21], Loss: 0.0010\n",
      "Epoch [124/200], Step [20/21], Loss: 0.0010\n",
      "Epoch [125/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [125/200], Step [20/21], Loss: 0.0008\n",
      "Epoch [126/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [126/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [127/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [127/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [128/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [128/200], Step [20/21], Loss: 0.0009\n",
      "Epoch [129/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [129/200], Step [20/21], Loss: 0.0010\n",
      "Epoch [130/200], Step [10/21], Loss: 0.0008\n",
      "Epoch [130/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [131/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [131/200], Step [20/21], Loss: 0.0006\n",
      "Epoch [132/200], Step [10/21], Loss: 0.0008\n",
      "Epoch [132/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [133/200], Step [10/21], Loss: 0.0010\n",
      "Epoch [133/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [134/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [134/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [135/200], Step [10/21], Loss: 0.0010\n",
      "Epoch [135/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [136/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [136/200], Step [20/21], Loss: 0.0010\n",
      "Epoch [137/200], Step [10/21], Loss: 0.0007\n",
      "Epoch [137/200], Step [20/21], Loss: 0.0008\n",
      "Epoch [138/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [138/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [139/200], Step [10/21], Loss: 0.0008\n",
      "Epoch [139/200], Step [20/21], Loss: 0.0006\n",
      "Epoch [140/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [140/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [141/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [141/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [142/200], Step [10/21], Loss: 0.0012\n",
      "Epoch [142/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [143/200], Step [10/21], Loss: 0.0007\n",
      "Epoch [143/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [144/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [144/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [145/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [145/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [146/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [146/200], Step [20/21], Loss: 0.0007\n",
      "Epoch [147/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [147/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [148/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [148/200], Step [20/21], Loss: 0.0009\n",
      "Epoch [149/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [149/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [150/200], Step [10/21], Loss: 0.0007\n",
      "Epoch [150/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [151/200], Step [10/21], Loss: 0.0009\n",
      "Epoch [151/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [152/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [152/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [153/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [153/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [154/200], Step [10/21], Loss: 0.0006\n",
      "Epoch [154/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [155/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [155/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [156/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [156/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [157/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [157/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [158/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [158/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [159/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [159/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [160/200], Step [10/21], Loss: 0.0008\n",
      "Epoch [160/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [161/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [161/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [162/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [162/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [163/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [163/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [164/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [164/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [165/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [165/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [166/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [166/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [167/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [167/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [168/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [168/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [169/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [169/200], Step [20/21], Loss: 0.0006\n",
      "Epoch [170/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [170/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [171/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [171/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [172/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [172/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [173/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [173/200], Step [20/21], Loss: 0.0005\n",
      "Epoch [174/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [174/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [175/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [175/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [176/200], Step [10/21], Loss: 0.0001\n",
      "Epoch [176/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [177/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [177/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [178/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [178/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [179/200], Step [10/21], Loss: 0.0004\n",
      "Epoch [179/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [180/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [180/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [181/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [181/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [182/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [182/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [183/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [183/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [184/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [184/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [185/200], Step [10/21], Loss: 0.0005\n",
      "Epoch [185/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [186/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [186/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [187/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [187/200], Step [20/21], Loss: 0.0004\n",
      "Epoch [188/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [188/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [189/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [189/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [190/200], Step [10/21], Loss: 0.0001\n",
      "Epoch [190/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [191/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [191/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [192/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [192/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [193/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [193/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [194/200], Step [10/21], Loss: 0.0001\n",
      "Epoch [194/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [195/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [195/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [196/200], Step [10/21], Loss: 0.0001\n",
      "Epoch [196/200], Step [20/21], Loss: 0.0001\n",
      "Epoch [197/200], Step [10/21], Loss: 0.0001\n",
      "Epoch [197/200], Step [20/21], Loss: 0.0003\n",
      "Epoch [198/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [198/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [199/200], Step [10/21], Loss: 0.0003\n",
      "Epoch [199/200], Step [20/21], Loss: 0.0002\n",
      "Epoch [200/200], Step [10/21], Loss: 0.0002\n",
      "Epoch [200/200], Step [20/21], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 参数设置\n",
    "input_size = 768  # 输入特征的维度\n",
    "hidden_size = 256  # GRU的隐藏层大小\n",
    "num_layers = 1  # GRU的层数\n",
    "num_classes = 3  # 输出类别数\n",
    "batch_size = 16  # 批大小\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 200  # 训练周期数\n",
    "\n",
    "# 确保CUDA可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型定义\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        scores = torch.matmul(hidden_states, self.weight)  # [batch_size, seq_len]\n",
    "        attn_weights = F.softmax(scores, dim=1)\n",
    "        context = torch.sum(hidden_states * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class BiGRUWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiGRUWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)  # *2 for bidirectional\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        context, attn_weights = self.attention(out)\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "\n",
    "# 初始化模型并移至GPU\n",
    "model = BiGRUWithAttention(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# 加载数据...\n",
    "train_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\processed\\updated_train.npy')\n",
    "train_labels = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\labels\\train_(327, 3).npy')\n",
    "train_dataset = TensorDataset(torch.Tensor(train_features), torch.Tensor(train_labels))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 损失函数和优化器...\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练循环...\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2963\n",
      "Macro F1 Score: 0.2713\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.50      0.38      0.43        40\n",
      "        Real       0.21      0.19      0.20        31\n",
      "  Unverified       0.13      0.30      0.18        10\n",
      "\n",
      "    accuracy                           0.30        81\n",
      "   macro avg       0.28      0.29      0.27        81\n",
      "weighted avg       0.35      0.30      0.31        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "test_features = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\processed\\updated_test.npy')\n",
    "test_labels = np.load(r'D:\\project\\WWW2021-master\\code\\preprocess\\data\\RumourEval-19\\labels\\test_(81, 3).npy')\n",
    "test_dataset = TensorDataset(torch.Tensor(test_features), torch.Tensor(test_labels))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# 计算准确率\n",
    "all_labels = np.argmax(all_labels, axis=1)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# 计算总体的F1分数\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Macro F1 Score: {f1:.4f}')\n",
    "# 输出分类报告，确保包含了所有类别\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real', 'Unverified']))\n",
    "# print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 初始化分词器和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 文本输入\n",
    "text = \"“: “: CharlieHebdo : “The cartoonists Charb & cabu are dead.”  ”” \"\n",
    "\n",
    "# 编码文本\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)  # 添加特殊令牌\n",
    "input_tensors = torch.tensor([input_ids])\n",
    "\n",
    "# 获取词嵌入\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensors)\n",
    "    last_hidden_states = outputs.last_hidden_state  # 最后一层的隐藏状态\n",
    "\n",
    "# 打印结果的shape以验证\n",
    "print(last_hidden_states.shape)\n",
    "# 输出的shape为 (1, N, 768)，其中N是输入文本的长度（包括特殊令牌）\n",
    "\n",
    "# 注意：实际的嵌入向量存储在`last_hidden_states`变量中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0537,  0.0580,  0.1904,  ..., -0.1097,  0.1455,  0.2739],\n",
       "         [ 0.1704, -0.0312,  0.2530,  ..., -0.0290,  0.4150,  0.2514],\n",
       "         [-0.0683, -0.0624,  0.1292,  ..., -0.0364,  0.1865,  0.4281],\n",
       "         ...,\n",
       "         [ 0.1863, -0.0891,  0.7164,  ...,  0.1588,  0.2346, -0.2987],\n",
       "         [-0.1325, -0.5134,  0.0344,  ...,  0.4457,  0.4667, -0.0658],\n",
       "         [ 0.7094,  1.0439,  0.0761,  ..., -0.0454, -0.5712, -0.0382]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (26) must match the size of tensor b (72) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_tensors)\n\u001b[0;32m     11\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# 最后一层的隐藏状态\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_hidden_states\u001b[49m\n\u001b[0;32m     14\u001b[0m b\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (26) must match the size of tensor b (72) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# 文本输入\n",
    "text = \"“@Colvinius: “@LePoint: #CharlieHebdo : “The cartoonists Charb &amp; cabu are dead.” http://t.co/DulWZcURCu http://t.co/vbQMAW7MC0”” @hyzaidi\"\n",
    "\n",
    "# 编码文本\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=True)  # 添加特殊令牌\n",
    "input_tensors = torch.tensor([input_ids])\n",
    "\n",
    "# 获取词嵌入\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensors)\n",
    "    last_hidden_states = outputs.last_hidden_state  # 最后一层的隐藏状态\n",
    "\n",
    "b = a - last_hidden_states\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.0417\n",
      "Epoch [2/200], Loss: 0.8492\n",
      "Epoch [3/200], Loss: 0.6195\n",
      "Epoch [4/200], Loss: 1.0046\n",
      "Epoch [5/200], Loss: 0.5041\n",
      "Epoch [6/200], Loss: 0.2513\n",
      "Epoch [7/200], Loss: 0.2607\n",
      "Epoch [8/200], Loss: 0.6506\n",
      "Epoch [9/200], Loss: 0.3046\n",
      "Epoch [10/200], Loss: 0.4258\n",
      "Epoch [11/200], Loss: 0.2583\n",
      "Epoch [12/200], Loss: 0.1412\n",
      "Epoch [13/200], Loss: 0.0648\n",
      "Epoch [14/200], Loss: 0.0196\n",
      "Epoch [15/200], Loss: 0.0110\n",
      "Epoch [16/200], Loss: 0.2881\n",
      "Epoch [17/200], Loss: 0.0178\n",
      "Epoch [18/200], Loss: 0.0054\n",
      "Epoch [19/200], Loss: 0.0050\n",
      "Epoch [20/200], Loss: 0.0019\n",
      "Epoch [21/200], Loss: 0.0008\n",
      "Epoch [22/200], Loss: 0.0008\n",
      "Epoch [23/200], Loss: 0.0015\n",
      "Epoch [24/200], Loss: 0.0024\n",
      "Epoch [25/200], Loss: 0.0004\n",
      "Epoch [26/200], Loss: 0.0002\n",
      "Epoch [27/200], Loss: 0.0009\n",
      "Epoch [28/200], Loss: 0.0008\n",
      "Epoch [29/200], Loss: 0.0012\n",
      "Epoch [30/200], Loss: 0.0018\n",
      "Epoch [31/200], Loss: 0.0002\n",
      "Epoch [32/200], Loss: 0.0004\n",
      "Epoch [33/200], Loss: 0.0002\n",
      "Epoch [34/200], Loss: 0.0003\n",
      "Epoch [35/200], Loss: 0.0001\n",
      "Epoch [36/200], Loss: 0.0001\n",
      "Epoch [37/200], Loss: 0.0003\n",
      "Epoch [38/200], Loss: 0.0010\n",
      "Epoch [39/200], Loss: 0.0009\n",
      "Epoch [40/200], Loss: 0.0001\n",
      "Epoch [41/200], Loss: 0.0003\n",
      "Epoch [42/200], Loss: 0.0003\n",
      "Epoch [43/200], Loss: 0.0003\n",
      "Epoch [44/200], Loss: 0.0001\n",
      "Epoch [45/200], Loss: 0.0002\n",
      "Epoch [46/200], Loss: 0.0002\n",
      "Epoch [47/200], Loss: 0.0004\n",
      "Epoch [48/200], Loss: 0.0001\n",
      "Epoch [49/200], Loss: 0.0001\n",
      "Epoch [50/200], Loss: 0.0001\n",
      "Epoch [51/200], Loss: 0.0001\n",
      "Epoch [52/200], Loss: 0.0002\n",
      "Epoch [53/200], Loss: 0.0001\n",
      "Epoch [54/200], Loss: 0.0002\n",
      "Epoch [55/200], Loss: 0.0001\n",
      "Epoch [56/200], Loss: 0.0001\n",
      "Epoch [57/200], Loss: 0.0002\n",
      "Epoch [58/200], Loss: 0.0001\n",
      "Epoch [59/200], Loss: 0.0001\n",
      "Epoch [60/200], Loss: 0.0002\n",
      "Epoch [61/200], Loss: 0.0004\n",
      "Epoch [62/200], Loss: 0.0001\n",
      "Epoch [63/200], Loss: 0.0002\n",
      "Epoch [64/200], Loss: 0.0001\n",
      "Epoch [65/200], Loss: 0.0002\n",
      "Epoch [66/200], Loss: 0.0000\n",
      "Epoch [67/200], Loss: 0.0001\n",
      "Epoch [68/200], Loss: 0.0001\n",
      "Epoch [69/200], Loss: 0.0000\n",
      "Epoch [70/200], Loss: 0.0002\n",
      "Epoch [71/200], Loss: 0.0001\n",
      "Epoch [72/200], Loss: 0.0001\n",
      "Epoch [73/200], Loss: 0.0000\n",
      "Epoch [74/200], Loss: 0.0001\n",
      "Epoch [75/200], Loss: 0.0001\n",
      "Epoch [76/200], Loss: 0.0000\n",
      "Epoch [77/200], Loss: 0.0000\n",
      "Epoch [78/200], Loss: 0.0000\n",
      "Epoch [79/200], Loss: 0.0001\n",
      "Epoch [80/200], Loss: 0.0001\n",
      "Epoch [81/200], Loss: 0.0001\n",
      "Epoch [82/200], Loss: 0.0001\n",
      "Epoch [83/200], Loss: 0.0001\n",
      "Epoch [84/200], Loss: 0.0001\n",
      "Epoch [85/200], Loss: 0.0000\n",
      "Epoch [86/200], Loss: 0.0001\n",
      "Epoch [87/200], Loss: 0.0001\n",
      "Epoch [88/200], Loss: 0.0001\n",
      "Epoch [89/200], Loss: 0.0000\n",
      "Epoch [90/200], Loss: 0.0000\n",
      "Epoch [91/200], Loss: 0.0000\n",
      "Epoch [92/200], Loss: 0.0000\n",
      "Epoch [93/200], Loss: 0.0000\n",
      "Epoch [94/200], Loss: 0.0000\n",
      "Epoch [95/200], Loss: 0.0001\n",
      "Epoch [96/200], Loss: 0.0000\n",
      "Epoch [97/200], Loss: 0.0000\n",
      "Epoch [98/200], Loss: 0.0001\n",
      "Epoch [99/200], Loss: 0.0000\n",
      "Epoch [100/200], Loss: 0.0000\n",
      "Epoch [101/200], Loss: 0.0000\n",
      "Epoch [102/200], Loss: 0.0000\n",
      "Epoch [103/200], Loss: 0.0000\n",
      "Epoch [104/200], Loss: 0.0000\n",
      "Epoch [105/200], Loss: 0.0000\n",
      "Epoch [106/200], Loss: 0.0000\n",
      "Epoch [107/200], Loss: 0.0000\n",
      "Epoch [108/200], Loss: 0.0000\n",
      "Epoch [109/200], Loss: 0.0000\n",
      "Epoch [110/200], Loss: 0.0001\n",
      "Epoch [111/200], Loss: 0.0000\n",
      "Epoch [112/200], Loss: 0.0000\n",
      "Epoch [113/200], Loss: 0.0000\n",
      "Epoch [114/200], Loss: 0.0000\n",
      "Epoch [115/200], Loss: 0.0000\n",
      "Epoch [116/200], Loss: 0.0000\n",
      "Epoch [117/200], Loss: 0.0000\n",
      "Epoch [118/200], Loss: 0.0000\n",
      "Epoch [119/200], Loss: 0.0000\n",
      "Epoch [120/200], Loss: 0.0000\n",
      "Epoch [121/200], Loss: 0.0000\n",
      "Epoch [122/200], Loss: 0.0000\n",
      "Epoch [123/200], Loss: 0.0000\n",
      "Epoch [124/200], Loss: 0.0000\n",
      "Epoch [125/200], Loss: 0.0000\n",
      "Epoch [126/200], Loss: 0.0000\n",
      "Epoch [127/200], Loss: 0.0000\n",
      "Epoch [128/200], Loss: 0.0000\n",
      "Epoch [129/200], Loss: 0.0000\n",
      "Epoch [130/200], Loss: 0.0000\n",
      "Epoch [131/200], Loss: 0.0000\n",
      "Epoch [132/200], Loss: 0.0000\n",
      "Epoch [133/200], Loss: 0.0000\n",
      "Epoch [134/200], Loss: 0.0000\n",
      "Epoch [135/200], Loss: 0.0000\n",
      "Epoch [136/200], Loss: 0.0000\n",
      "Epoch [137/200], Loss: 0.0000\n",
      "Epoch [138/200], Loss: 0.0000\n",
      "Epoch [139/200], Loss: 0.0000\n",
      "Epoch [140/200], Loss: 0.0000\n",
      "Epoch [141/200], Loss: 0.0000\n",
      "Epoch [142/200], Loss: 0.0000\n",
      "Epoch [143/200], Loss: 0.0000\n",
      "Epoch [144/200], Loss: 0.0000\n",
      "Epoch [145/200], Loss: 0.0000\n",
      "Epoch [146/200], Loss: 0.0000\n",
      "Epoch [147/200], Loss: 0.0000\n",
      "Epoch [148/200], Loss: 0.0000\n",
      "Epoch [149/200], Loss: 0.0000\n",
      "Epoch [150/200], Loss: 0.0000\n",
      "Epoch [151/200], Loss: 0.0000\n",
      "Epoch [152/200], Loss: 0.0000\n",
      "Epoch [153/200], Loss: 0.0000\n",
      "Epoch [154/200], Loss: 0.0000\n",
      "Epoch [155/200], Loss: 0.0000\n",
      "Epoch [156/200], Loss: 0.0000\n",
      "Epoch [157/200], Loss: 0.0000\n",
      "Epoch [158/200], Loss: 0.0000\n",
      "Epoch [159/200], Loss: 0.0000\n",
      "Epoch [160/200], Loss: 0.0000\n",
      "Epoch [161/200], Loss: 0.0000\n",
      "Epoch [162/200], Loss: 0.0000\n",
      "Epoch [163/200], Loss: 0.0000\n",
      "Epoch [164/200], Loss: 0.0000\n",
      "Epoch [165/200], Loss: 0.0000\n",
      "Epoch [166/200], Loss: 0.0000\n",
      "Epoch [167/200], Loss: 0.0000\n",
      "Epoch [168/200], Loss: 0.0000\n",
      "Epoch [169/200], Loss: 0.0000\n",
      "Epoch [170/200], Loss: 0.0000\n",
      "Epoch [171/200], Loss: 0.0000\n",
      "Epoch [172/200], Loss: 0.0000\n",
      "Epoch [173/200], Loss: 0.0000\n",
      "Epoch [174/200], Loss: 0.0000\n",
      "Epoch [175/200], Loss: 0.0000\n",
      "Epoch [176/200], Loss: 0.0000\n",
      "Epoch [177/200], Loss: 0.0000\n",
      "Epoch [178/200], Loss: 0.0000\n",
      "Epoch [179/200], Loss: 0.0000\n",
      "Epoch [180/200], Loss: 0.0000\n",
      "Epoch [181/200], Loss: 0.0000\n",
      "Epoch [182/200], Loss: 0.0000\n",
      "Epoch [183/200], Loss: 0.0000\n",
      "Epoch [184/200], Loss: 0.0000\n",
      "Epoch [185/200], Loss: 0.0000\n",
      "Epoch [186/200], Loss: 0.0000\n",
      "Epoch [187/200], Loss: 0.0000\n",
      "Epoch [188/200], Loss: 0.0000\n",
      "Epoch [189/200], Loss: 0.0000\n",
      "Epoch [190/200], Loss: 0.0000\n",
      "Epoch [191/200], Loss: 0.0000\n",
      "Epoch [192/200], Loss: 0.0000\n",
      "Epoch [193/200], Loss: 0.0000\n",
      "Epoch [194/200], Loss: 0.0000\n",
      "Epoch [195/200], Loss: 0.0000\n",
      "Epoch [196/200], Loss: 0.0000\n",
      "Epoch [197/200], Loss: 0.0000\n",
      "Epoch [198/200], Loss: 0.0000\n",
      "Epoch [199/200], Loss: 0.0000\n",
      "Epoch [200/200], Loss: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.53      0.53        40\n",
      "           1       0.23      0.16      0.19        31\n",
      "           2       0.05      0.10      0.07        10\n",
      "\n",
      "    accuracy                           0.33        81\n",
      "   macro avg       0.27      0.26      0.26        81\n",
      "weighted avg       0.36      0.33      0.34        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 考虑到BiGRU是用于处理序列数据，我们可以将特征向量视为序列的一个时间步。\n",
    "# 这意味着我们需要调整数据的形状以适配BiGRU模型的输入要求。\n",
    "# 为了使用BiGRU处理非序列化的特征数据，我们可以将每个特征向量\"伪装\"成长度为1的序列。\n",
    "# 下面是调整后的BiGRU模型训练和评估代码：\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 定义BiGRU模型\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 2  # 可以调整层数\n",
    "        self.gru = nn.GRU(input_size, hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 因为是双向的，所以是hidden_size * 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        # 前向传播\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # 解码最后一个时间步的隐藏状态\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 加载数据并调整形状以适应BiGRU\n",
    "train_features = np.load(r'D:/project/WWW2021-master/code/preprocess/train_features.npy')\n",
    "train_labels = np.argmax(np.load('D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/train_(327, 3).npy'), axis=1)\n",
    "test_features = np.load(r'D:/project/WWW2021-master/code/preprocess/test_features.npy')\n",
    "test_labels = np.argmax(np.load('D:/project/WWW2021-master/code/preprocess/data/RumourEval-19/labels/test_(81, 3).npy'), axis=1)\n",
    "\n",
    "# 将特征向量\"伪装\"成长度为1的序列\n",
    "train_features = np.expand_dims(train_features, axis=1)\n",
    "test_features = np.expand_dims(test_features, axis=1)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化BiGRU模型、损失函数和优化器\n",
    "model = BiGRU(input_size=train_features.shape[2], hidden_size=128, num_classes=3).to(train_features.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(train_features.device)\n",
    "        labels = labels.to(train_features.device)\n",
    "        # 前向传播\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features = features.to(test_features.device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(classification_report(test_labels.numpy(), np.array(all_preds)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
